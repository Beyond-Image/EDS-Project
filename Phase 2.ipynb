{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6407d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Felipe\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imblearn\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, LassoCV, SGDClassifier, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif, chi2, SequentialFeatureSelector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_rows', 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b42dee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\felipe\\anaconda3\\lib\\site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 19.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-23.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.11.exe, pip3.9.exe and pip3.exe are installed in 'C:\\Users\\Felipe\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a47c",
   "metadata": {},
   "source": [
    "# Creating Cleaned and Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da325b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70692 entries, 0 to 70691\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   HighBP                70692 non-null  float64\n",
      " 1   HighChol              70692 non-null  float64\n",
      " 2   CholCheck             70692 non-null  float64\n",
      " 3   BMI                   70692 non-null  float64\n",
      " 4   Smoker                70692 non-null  float64\n",
      " 5   Stroke                70692 non-null  float64\n",
      " 6   HeartDiseaseorAttack  70692 non-null  float64\n",
      " 7   PhysActivity          70692 non-null  float64\n",
      " 8   Fruits                70692 non-null  float64\n",
      " 9   Veggies               70692 non-null  float64\n",
      " 10  HvyAlcoholConsump     70692 non-null  float64\n",
      " 11  AnyHealthcare         70692 non-null  float64\n",
      " 12  NoDocbcCost           70692 non-null  float64\n",
      " 13  GenHlth               70692 non-null  float64\n",
      " 14  MentHlth              70692 non-null  float64\n",
      " 15  PhysHlth              70692 non-null  float64\n",
      " 16  DiffWalk              70692 non-null  float64\n",
      " 17  Sex                   70692 non-null  float64\n",
      " 18  Age                   70692 non-null  float64\n",
      " 19  Education             70692 non-null  float64\n",
      " 20  Income                70692 non-null  float64\n",
      "dtypes: float64(21)\n",
      "memory usage: 11.3 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70692 entries, 0 to 70691\n",
      "Data columns (total 1 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Diabetes_binary  70692 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 552.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##Recreating the cleaned Dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "undersample = NearMiss(version=1)\n",
    "X = df.loc[:, df.columns != 'Diabetes_binary']\n",
    "y = df.loc[:, df.columns == 'Diabetes_binary']\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "print(X.info())\n",
    "print(y.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c18342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2805b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-1.221972</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>-0.514634</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.356032</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>0.697964</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>2.603001</td>\n",
       "      <td>-1.942597</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>-2.390830</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>1.255723</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>2.060543</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>-1.162526</td>\n",
       "      <td>-2.571665</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-1.061978</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>-1.494245</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>-0.073432</td>\n",
       "      <td>-0.149059</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-0.581994</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-1.587783</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>0.808970</td>\n",
       "      <td>-0.149059</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>1.177948</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>-1.942597</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>-2.390830</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>2.664163</td>\n",
       "      <td>0.116531</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>-0.955835</td>\n",
       "      <td>-1.162526</td>\n",
       "      <td>-0.131917</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49479</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-0.262004</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>-1.942597</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>0.307887</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>0.808970</td>\n",
       "      <td>-1.162526</td>\n",
       "      <td>-0.619867</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49480</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-0.262004</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>-1.494245</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>-0.955835</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49481</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-1.061978</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>4.432171</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.220989</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>0.808970</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>-1.107816</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49482</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>0.057985</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>-0.073432</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49483</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>0.857959</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>-1.838238</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49484 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HighBP  HighChol  CholCheck       BMI    Smoker    Stroke  \\\n",
       "0     -1.212894 -1.140353   0.074482 -1.221972  1.158253 -0.225623   \n",
       "1      0.824475  0.876922   0.074482  0.697964  1.158253 -0.225623   \n",
       "2     -1.212894 -1.140353   0.074482 -1.061978  1.158253 -0.225623   \n",
       "3      0.824475 -1.140353   0.074482 -0.581994 -0.863369 -0.225623   \n",
       "4      0.824475 -1.140353   0.074482  1.177948  1.158253 -0.225623   \n",
       "...         ...       ...        ...       ...       ...       ...   \n",
       "49479  0.824475 -1.140353   0.074482 -0.262004  1.158253 -0.225623   \n",
       "49480 -1.212894  0.876922   0.074482 -0.262004 -0.863369 -0.225623   \n",
       "49481  0.824475  0.876922   0.074482 -1.061978 -0.863369  4.432171   \n",
       "49482 -1.212894 -1.140353   0.074482  0.057985 -0.863369 -0.225623   \n",
       "49483  0.824475 -1.140353   0.074482  0.857959 -0.863369 -0.225623   \n",
       "\n",
       "       HeartDiseaseorAttack  PhysActivity    Fruits   Veggies  \\\n",
       "0                 -0.384172      0.514775  0.669234  0.418265   \n",
       "1                  2.603001     -1.942597  0.669234 -2.390830   \n",
       "2                 -0.384172      0.514775 -1.494245  0.418265   \n",
       "3                 -0.384172      0.514775  0.669234  0.418265   \n",
       "4                 -0.384172     -1.942597  0.669234 -2.390830   \n",
       "...                     ...           ...       ...       ...   \n",
       "49479             -0.384172     -1.942597  0.669234  0.418265   \n",
       "49480             -0.384172      0.514775 -1.494245  0.418265   \n",
       "49481             -0.384172      0.514775  0.669234  0.418265   \n",
       "49482             -0.384172      0.514775  0.669234  0.418265   \n",
       "49483             -0.384172      0.514775  0.669234  0.418265   \n",
       "\n",
       "       HvyAlcoholConsump  AnyHealthcare  NoDocbcCost   GenHlth  MentHlth  \\\n",
       "0              -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "1              -0.139079       0.147309    -0.237966  1.255723 -0.332699   \n",
       "2              -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "3              -0.139079       0.147309    -0.237966 -1.587783 -0.332699   \n",
       "4              -0.139079       0.147309    -0.237966 -0.639948  2.664163   \n",
       "...                  ...            ...          ...       ...       ...   \n",
       "49479          -0.139079       0.147309    -0.237966  0.307887 -0.332699   \n",
       "49480          -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "49481          -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "49482          -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "49483          -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "\n",
       "       PhysHlth  DiffWalk       Sex       Age  Education    Income  \\\n",
       "0     -0.446002 -0.485309 -1.036097 -0.514634   0.864409  0.356032   \n",
       "1     -0.446002  2.060543 -1.036097  0.367769  -1.162526 -2.571665   \n",
       "2     -0.446002 -0.485309  0.965161 -0.073432  -0.149059  0.843982   \n",
       "3     -0.446002 -0.485309 -1.036097  0.808970  -0.149059  0.843982   \n",
       "4      0.116531 -0.485309 -1.036097 -0.955835  -1.162526 -0.131917   \n",
       "...         ...       ...       ...       ...        ...       ...   \n",
       "49479 -0.446002 -0.485309  0.965161  0.808970  -1.162526 -0.619867   \n",
       "49480 -0.446002 -0.485309  0.965161 -0.955835   0.864409  0.843982   \n",
       "49481 -0.220989 -0.485309 -1.036097  0.808970   0.864409 -1.107816   \n",
       "49482 -0.446002 -0.485309  0.965161 -0.073432   0.864409  0.843982   \n",
       "49483 -0.446002 -0.485309  0.965161 -1.838238   0.864409  0.843982   \n",
       "\n",
       "       Diabetes_binary  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  NaN  \n",
       "...                ...  \n",
       "49479              1.0  \n",
       "49480              1.0  \n",
       "49481              1.0  \n",
       "49482              1.0  \n",
       "49483              1.0  \n",
       "\n",
       "[49484 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_undersampled_train = pd.DataFrame(X_train_scaled, columns = X.columns)\n",
    "df_undersampled_train['Diabetes_binary'] = y_train\n",
    "df_undersampled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5defc5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-1.061978</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>-1.494245</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>1.255723</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>-1.162526</td>\n",
       "      <td>-0.619867</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>0.377975</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>-2.279439</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>1.017954</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>0.307887</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.220989</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>-0.073432</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>0.377975</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>-0.514634</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.356032</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>2.777896</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>1.255723</td>\n",
       "      <td>4.162593</td>\n",
       "      <td>0.679064</td>\n",
       "      <td>2.060543</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>-1.397036</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>-1.595766</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21203</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-0.262004</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21204</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>4.857827</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>-1.942597</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>1.255723</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>2.060543</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>-1.107816</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21205</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>-1.140353</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-1.221972</td>\n",
       "      <td>-0.863369</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>-1.494245</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>-0.139079</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.639948</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>-1.036097</td>\n",
       "      <td>-0.073432</td>\n",
       "      <td>0.864409</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21206</th>\n",
       "      <td>-1.212894</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>-0.581994</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>7.190175</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>2.203558</td>\n",
       "      <td>4.162593</td>\n",
       "      <td>0.454051</td>\n",
       "      <td>2.060543</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>-1.838238</td>\n",
       "      <td>-0.149059</td>\n",
       "      <td>-2.571665</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21207</th>\n",
       "      <td>0.824475</td>\n",
       "      <td>0.876922</td>\n",
       "      <td>0.074482</td>\n",
       "      <td>0.537969</td>\n",
       "      <td>1.158253</td>\n",
       "      <td>-0.225623</td>\n",
       "      <td>-0.384172</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>-1.494245</td>\n",
       "      <td>0.418265</td>\n",
       "      <td>7.190175</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>1.255723</td>\n",
       "      <td>-0.332699</td>\n",
       "      <td>-0.446002</td>\n",
       "      <td>-0.485309</td>\n",
       "      <td>0.965161</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>-1.162526</td>\n",
       "      <td>0.843982</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21208 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HighBP  HighChol  CholCheck       BMI    Smoker    Stroke  \\\n",
       "0     -1.212894  0.876922   0.074482 -1.061978  1.158253 -0.225623   \n",
       "1     -1.212894 -1.140353   0.074482  0.377975  1.158253 -0.225623   \n",
       "2      0.824475  0.876922   0.074482  1.017954 -0.863369 -0.225623   \n",
       "3     -1.212894  0.876922   0.074482  0.377975 -0.863369 -0.225623   \n",
       "4      0.824475 -1.140353   0.074482  2.777896 -0.863369 -0.225623   \n",
       "...         ...       ...        ...       ...       ...       ...   \n",
       "21203  0.824475  0.876922   0.074482 -0.262004  1.158253 -0.225623   \n",
       "21204 -1.212894 -1.140353   0.074482  4.857827 -0.863369 -0.225623   \n",
       "21205 -1.212894 -1.140353   0.074482 -1.221972 -0.863369 -0.225623   \n",
       "21206 -1.212894  0.876922   0.074482 -0.581994  1.158253 -0.225623   \n",
       "21207  0.824475  0.876922   0.074482  0.537969  1.158253 -0.225623   \n",
       "\n",
       "       HeartDiseaseorAttack  PhysActivity    Fruits   Veggies  \\\n",
       "0                 -0.384172      0.514775 -1.494245  0.418265   \n",
       "1                 -0.384172      0.514775  0.669234  0.418265   \n",
       "2                 -0.384172      0.514775  0.669234  0.418265   \n",
       "3                 -0.384172      0.514775  0.669234  0.418265   \n",
       "4                 -0.384172      0.514775  0.669234  0.418265   \n",
       "...                     ...           ...       ...       ...   \n",
       "21203             -0.384172      0.514775  0.669234  0.418265   \n",
       "21204             -0.384172     -1.942597  0.669234  0.418265   \n",
       "21205             -0.384172      0.514775 -1.494245  0.418265   \n",
       "21206             -0.384172      0.514775  0.669234  0.418265   \n",
       "21207             -0.384172      0.514775 -1.494245  0.418265   \n",
       "\n",
       "       HvyAlcoholConsump  AnyHealthcare  NoDocbcCost   GenHlth  MentHlth  \\\n",
       "0              -0.139079       0.147309    -0.237966  1.255723 -0.332699   \n",
       "1              -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "2              -0.139079       0.147309    -0.237966  0.307887 -0.332699   \n",
       "3              -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "4              -0.139079       0.147309    -0.237966  1.255723  4.162593   \n",
       "...                  ...            ...          ...       ...       ...   \n",
       "21203          -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "21204          -0.139079       0.147309    -0.237966  1.255723 -0.332699   \n",
       "21205          -0.139079       0.147309    -0.237966 -0.639948 -0.332699   \n",
       "21206           7.190175       0.147309    -0.237966  2.203558  4.162593   \n",
       "21207           7.190175       0.147309    -0.237966  1.255723 -0.332699   \n",
       "\n",
       "       PhysHlth  DiffWalk       Sex       Age  Education    Income  \\\n",
       "0     -0.446002 -0.485309 -1.036097  0.367769  -1.162526 -0.619867   \n",
       "1     -0.446002 -0.485309  0.965161 -2.279439   0.864409  0.843982   \n",
       "2     -0.220989 -0.485309  0.965161 -0.073432   0.864409  0.843982   \n",
       "3     -0.446002 -0.485309 -1.036097 -0.514634   0.864409  0.356032   \n",
       "4      0.679064  2.060543  0.965161 -1.397036   0.864409 -1.595766   \n",
       "...         ...       ...       ...       ...        ...       ...   \n",
       "21203 -0.446002 -0.485309  0.965161  0.367769   0.864409  0.843982   \n",
       "21204 -0.446002  2.060543 -1.036097  0.367769   0.864409 -1.107816   \n",
       "21205 -0.446002 -0.485309 -1.036097 -0.073432   0.864409  0.843982   \n",
       "21206  0.454051  2.060543  0.965161 -1.838238  -0.149059 -2.571665   \n",
       "21207 -0.446002 -0.485309  0.965161  0.367769  -1.162526  0.843982   \n",
       "\n",
       "       Diabetes_binary  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  0.0  \n",
       "...                ...  \n",
       "21203              NaN  \n",
       "21204              0.0  \n",
       "21205              NaN  \n",
       "21206              NaN  \n",
       "21207              NaN  \n",
       "\n",
       "[21208 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_undersampled_test = pd.DataFrame(X_test_scaled, columns = X.columns)\n",
    "df_undersampled_test['Diabetes_binary'] = y_test\n",
    "df_undersampled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84987478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1571: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HighBP: 0.031\n",
      "HighChol: 0.013\n",
      "BMI: 0.065\n",
      "Smoker: 0.016\n",
      "Stroke: 0.015\n",
      "HeartDiseaseorAttack: 0.029\n",
      "HvyAlcoholConsump: 0.017\n",
      "NoDocbcCost: 0.015\n",
      "GenHlth: 0.11\n",
      "MentHlth: 0.016\n",
      "PhysHlth: 0.024\n",
      "DiffWalk: 0.03\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV(cv=5, random_state=0).fit(X_train_scaled, y_train)\n",
    "\n",
    "coef = lasso.coef_\n",
    "col = X.columns\n",
    "for index in range(len(coef)):\n",
    "    if coef[index] > 0.01:\n",
    "        print(f'{col[index]}: {np.round(coef[index], 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484b0e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HighBP</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.290281</td>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.252404</td>\n",
       "      <td>0.132512</td>\n",
       "      <td>0.119859</td>\n",
       "      <td>0.192153</td>\n",
       "      <td>-0.185341</td>\n",
       "      <td>-0.103985</td>\n",
       "      <td>-0.123597</td>\n",
       "      <td>0.022629</td>\n",
       "      <td>-0.032929</td>\n",
       "      <td>0.077367</td>\n",
       "      <td>0.322079</td>\n",
       "      <td>0.124688</td>\n",
       "      <td>0.188203</td>\n",
       "      <td>0.225602</td>\n",
       "      <td>-0.011667</td>\n",
       "      <td>0.275838</td>\n",
       "      <td>-0.227568</td>\n",
       "      <td>-0.282530</td>\n",
       "      <td>-0.012546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HighChol</th>\n",
       "      <td>0.290281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012837</td>\n",
       "      <td>0.129130</td>\n",
       "      <td>0.125240</td>\n",
       "      <td>0.081647</td>\n",
       "      <td>0.160192</td>\n",
       "      <td>-0.123335</td>\n",
       "      <td>-0.089436</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>0.028740</td>\n",
       "      <td>-0.023433</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.223396</td>\n",
       "      <td>0.117957</td>\n",
       "      <td>0.139995</td>\n",
       "      <td>0.147227</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>0.163192</td>\n",
       "      <td>-0.134820</td>\n",
       "      <td>-0.163382</td>\n",
       "      <td>-0.011030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CholCheck</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.012837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>-0.005190</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>0.015673</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>0.079857</td>\n",
       "      <td>-0.059807</td>\n",
       "      <td>-0.020087</td>\n",
       "      <td>-0.027875</td>\n",
       "      <td>-0.014818</td>\n",
       "      <td>-0.009001</td>\n",
       "      <td>-0.010735</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.031126</td>\n",
       "      <td>-0.002274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.252404</td>\n",
       "      <td>0.129130</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063508</td>\n",
       "      <td>0.057133</td>\n",
       "      <td>0.098065</td>\n",
       "      <td>-0.255187</td>\n",
       "      <td>-0.158303</td>\n",
       "      <td>-0.123689</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>-0.064963</td>\n",
       "      <td>0.129655</td>\n",
       "      <td>0.345232</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.255436</td>\n",
       "      <td>0.316306</td>\n",
       "      <td>-0.044020</td>\n",
       "      <td>-0.112717</td>\n",
       "      <td>-0.201685</td>\n",
       "      <td>-0.242094</td>\n",
       "      <td>-0.004097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoker</th>\n",
       "      <td>0.132512</td>\n",
       "      <td>0.125240</td>\n",
       "      <td>-0.005190</td>\n",
       "      <td>0.063508</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072942</td>\n",
       "      <td>0.144389</td>\n",
       "      <td>-0.102905</td>\n",
       "      <td>-0.102695</td>\n",
       "      <td>-0.060063</td>\n",
       "      <td>0.066169</td>\n",
       "      <td>-0.028095</td>\n",
       "      <td>0.042161</td>\n",
       "      <td>0.181521</td>\n",
       "      <td>0.108281</td>\n",
       "      <td>0.140699</td>\n",
       "      <td>0.140045</td>\n",
       "      <td>0.115278</td>\n",
       "      <td>0.145365</td>\n",
       "      <td>-0.171960</td>\n",
       "      <td>-0.152527</td>\n",
       "      <td>-0.006953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stroke</th>\n",
       "      <td>0.119859</td>\n",
       "      <td>0.081647</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>0.057133</td>\n",
       "      <td>0.072942</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.233298</td>\n",
       "      <td>-0.126957</td>\n",
       "      <td>-0.046131</td>\n",
       "      <td>-0.084637</td>\n",
       "      <td>-0.008613</td>\n",
       "      <td>-0.028131</td>\n",
       "      <td>0.077127</td>\n",
       "      <td>0.223366</td>\n",
       "      <td>0.141691</td>\n",
       "      <td>0.210303</td>\n",
       "      <td>0.236295</td>\n",
       "      <td>-0.019964</td>\n",
       "      <td>0.083517</td>\n",
       "      <td>-0.120345</td>\n",
       "      <td>-0.194099</td>\n",
       "      <td>-0.002408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <td>0.192153</td>\n",
       "      <td>0.160192</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>0.098065</td>\n",
       "      <td>0.144389</td>\n",
       "      <td>0.233298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.147135</td>\n",
       "      <td>-0.063526</td>\n",
       "      <td>-0.085198</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>-0.021283</td>\n",
       "      <td>0.083611</td>\n",
       "      <td>0.311562</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>0.246269</td>\n",
       "      <td>0.271997</td>\n",
       "      <td>0.073961</td>\n",
       "      <td>0.173327</td>\n",
       "      <td>-0.153377</td>\n",
       "      <td>-0.216515</td>\n",
       "      <td>-0.004789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhysActivity</th>\n",
       "      <td>-0.185341</td>\n",
       "      <td>-0.123335</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>-0.255187</td>\n",
       "      <td>-0.102905</td>\n",
       "      <td>-0.126957</td>\n",
       "      <td>-0.147135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165934</td>\n",
       "      <td>0.204442</td>\n",
       "      <td>-0.010663</td>\n",
       "      <td>0.072273</td>\n",
       "      <td>-0.120715</td>\n",
       "      <td>-0.372282</td>\n",
       "      <td>-0.224465</td>\n",
       "      <td>-0.333044</td>\n",
       "      <td>-0.363598</td>\n",
       "      <td>0.081863</td>\n",
       "      <td>-0.080338</td>\n",
       "      <td>0.273416</td>\n",
       "      <td>0.307727</td>\n",
       "      <td>0.002891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fruits</th>\n",
       "      <td>-0.103985</td>\n",
       "      <td>-0.089436</td>\n",
       "      <td>0.015673</td>\n",
       "      <td>-0.158303</td>\n",
       "      <td>-0.102695</td>\n",
       "      <td>-0.046131</td>\n",
       "      <td>-0.063526</td>\n",
       "      <td>0.165934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245612</td>\n",
       "      <td>-0.027089</td>\n",
       "      <td>0.046059</td>\n",
       "      <td>-0.070790</td>\n",
       "      <td>-0.189348</td>\n",
       "      <td>-0.103770</td>\n",
       "      <td>-0.113491</td>\n",
       "      <td>-0.116434</td>\n",
       "      <td>-0.096090</td>\n",
       "      <td>0.042817</td>\n",
       "      <td>0.145128</td>\n",
       "      <td>0.140828</td>\n",
       "      <td>0.006887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Veggies</th>\n",
       "      <td>-0.123597</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>-0.123689</td>\n",
       "      <td>-0.060063</td>\n",
       "      <td>-0.084637</td>\n",
       "      <td>-0.085198</td>\n",
       "      <td>0.204442</td>\n",
       "      <td>0.245612</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.057291</td>\n",
       "      <td>-0.088002</td>\n",
       "      <td>-0.214869</td>\n",
       "      <td>-0.119102</td>\n",
       "      <td>-0.151267</td>\n",
       "      <td>-0.165544</td>\n",
       "      <td>-0.027881</td>\n",
       "      <td>-0.029046</td>\n",
       "      <td>0.214979</td>\n",
       "      <td>0.238881</td>\n",
       "      <td>-0.001889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <td>0.022629</td>\n",
       "      <td>0.028740</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.066169</td>\n",
       "      <td>-0.008613</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>-0.010663</td>\n",
       "      <td>-0.027089</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023685</td>\n",
       "      <td>0.012283</td>\n",
       "      <td>0.013480</td>\n",
       "      <td>0.025015</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>-0.003393</td>\n",
       "      <td>0.028414</td>\n",
       "      <td>-0.021322</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>-0.001104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <td>-0.032929</td>\n",
       "      <td>-0.023433</td>\n",
       "      <td>0.079857</td>\n",
       "      <td>-0.064963</td>\n",
       "      <td>-0.028095</td>\n",
       "      <td>-0.028131</td>\n",
       "      <td>-0.021283</td>\n",
       "      <td>0.072273</td>\n",
       "      <td>0.046059</td>\n",
       "      <td>0.057291</td>\n",
       "      <td>-0.023685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.231933</td>\n",
       "      <td>-0.107130</td>\n",
       "      <td>-0.095317</td>\n",
       "      <td>-0.068226</td>\n",
       "      <td>-0.053400</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>0.087374</td>\n",
       "      <td>0.125511</td>\n",
       "      <td>0.157447</td>\n",
       "      <td>0.007229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <td>0.077367</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>-0.059807</td>\n",
       "      <td>0.129655</td>\n",
       "      <td>0.042161</td>\n",
       "      <td>0.077127</td>\n",
       "      <td>0.083611</td>\n",
       "      <td>-0.120715</td>\n",
       "      <td>-0.070790</td>\n",
       "      <td>-0.088002</td>\n",
       "      <td>0.012283</td>\n",
       "      <td>-0.231933</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.231035</td>\n",
       "      <td>0.242863</td>\n",
       "      <td>0.221550</td>\n",
       "      <td>0.197706</td>\n",
       "      <td>-0.062672</td>\n",
       "      <td>-0.103265</td>\n",
       "      <td>-0.151419</td>\n",
       "      <td>-0.249087</td>\n",
       "      <td>0.004033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GenHlth</th>\n",
       "      <td>0.322079</td>\n",
       "      <td>0.223396</td>\n",
       "      <td>-0.020087</td>\n",
       "      <td>0.345232</td>\n",
       "      <td>0.181521</td>\n",
       "      <td>0.223366</td>\n",
       "      <td>0.311562</td>\n",
       "      <td>-0.372282</td>\n",
       "      <td>-0.189348</td>\n",
       "      <td>-0.214869</td>\n",
       "      <td>0.013480</td>\n",
       "      <td>-0.107130</td>\n",
       "      <td>0.231035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.402433</td>\n",
       "      <td>0.587633</td>\n",
       "      <td>0.517395</td>\n",
       "      <td>-0.087672</td>\n",
       "      <td>0.056083</td>\n",
       "      <td>-0.408064</td>\n",
       "      <td>-0.506399</td>\n",
       "      <td>-0.005024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MentHlth</th>\n",
       "      <td>0.124688</td>\n",
       "      <td>0.117957</td>\n",
       "      <td>-0.027875</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.108281</td>\n",
       "      <td>0.141691</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>-0.224465</td>\n",
       "      <td>-0.103770</td>\n",
       "      <td>-0.119102</td>\n",
       "      <td>0.025015</td>\n",
       "      <td>-0.095317</td>\n",
       "      <td>0.242863</td>\n",
       "      <td>0.402433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.480460</td>\n",
       "      <td>0.359089</td>\n",
       "      <td>-0.103016</td>\n",
       "      <td>-0.089631</td>\n",
       "      <td>-0.205666</td>\n",
       "      <td>-0.321325</td>\n",
       "      <td>-0.011795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhysHlth</th>\n",
       "      <td>0.188203</td>\n",
       "      <td>0.139995</td>\n",
       "      <td>-0.014818</td>\n",
       "      <td>0.255436</td>\n",
       "      <td>0.140699</td>\n",
       "      <td>0.210303</td>\n",
       "      <td>0.246269</td>\n",
       "      <td>-0.333044</td>\n",
       "      <td>-0.113491</td>\n",
       "      <td>-0.151267</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>-0.068226</td>\n",
       "      <td>0.221550</td>\n",
       "      <td>0.587633</td>\n",
       "      <td>0.480460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551986</td>\n",
       "      <td>-0.079606</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>-0.262703</td>\n",
       "      <td>-0.395411</td>\n",
       "      <td>-0.004940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiffWalk</th>\n",
       "      <td>0.225602</td>\n",
       "      <td>0.147227</td>\n",
       "      <td>-0.009001</td>\n",
       "      <td>0.316306</td>\n",
       "      <td>0.140045</td>\n",
       "      <td>0.236295</td>\n",
       "      <td>0.271997</td>\n",
       "      <td>-0.363598</td>\n",
       "      <td>-0.116434</td>\n",
       "      <td>-0.165544</td>\n",
       "      <td>-0.003393</td>\n",
       "      <td>-0.053400</td>\n",
       "      <td>0.197706</td>\n",
       "      <td>0.517395</td>\n",
       "      <td>0.359089</td>\n",
       "      <td>0.551986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.121975</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>-0.297588</td>\n",
       "      <td>-0.448740</td>\n",
       "      <td>-0.012193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>-0.011667</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>-0.010735</td>\n",
       "      <td>-0.044020</td>\n",
       "      <td>0.115278</td>\n",
       "      <td>-0.019964</td>\n",
       "      <td>0.073961</td>\n",
       "      <td>0.081863</td>\n",
       "      <td>-0.096090</td>\n",
       "      <td>-0.027881</td>\n",
       "      <td>0.028414</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>-0.062672</td>\n",
       "      <td>-0.087672</td>\n",
       "      <td>-0.103016</td>\n",
       "      <td>-0.079606</td>\n",
       "      <td>-0.121975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014452</td>\n",
       "      <td>0.117552</td>\n",
       "      <td>0.217407</td>\n",
       "      <td>0.012702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.275838</td>\n",
       "      <td>0.163192</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>-0.112717</td>\n",
       "      <td>0.145365</td>\n",
       "      <td>0.083517</td>\n",
       "      <td>0.173327</td>\n",
       "      <td>-0.080338</td>\n",
       "      <td>0.042817</td>\n",
       "      <td>-0.029046</td>\n",
       "      <td>-0.021322</td>\n",
       "      <td>0.087374</td>\n",
       "      <td>-0.103265</td>\n",
       "      <td>0.056083</td>\n",
       "      <td>-0.089631</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>-0.014452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124945</td>\n",
       "      <td>-0.178868</td>\n",
       "      <td>-0.003078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>-0.227568</td>\n",
       "      <td>-0.134820</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>-0.201685</td>\n",
       "      <td>-0.171960</td>\n",
       "      <td>-0.120345</td>\n",
       "      <td>-0.153377</td>\n",
       "      <td>0.273416</td>\n",
       "      <td>0.145128</td>\n",
       "      <td>0.214979</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.125511</td>\n",
       "      <td>-0.151419</td>\n",
       "      <td>-0.408064</td>\n",
       "      <td>-0.205666</td>\n",
       "      <td>-0.262703</td>\n",
       "      <td>-0.297588</td>\n",
       "      <td>0.117552</td>\n",
       "      <td>-0.124945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565459</td>\n",
       "      <td>0.004480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Income</th>\n",
       "      <td>-0.282530</td>\n",
       "      <td>-0.163382</td>\n",
       "      <td>0.031126</td>\n",
       "      <td>-0.242094</td>\n",
       "      <td>-0.152527</td>\n",
       "      <td>-0.194099</td>\n",
       "      <td>-0.216515</td>\n",
       "      <td>0.307727</td>\n",
       "      <td>0.140828</td>\n",
       "      <td>0.238881</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.157447</td>\n",
       "      <td>-0.249087</td>\n",
       "      <td>-0.506399</td>\n",
       "      <td>-0.321325</td>\n",
       "      <td>-0.395411</td>\n",
       "      <td>-0.448740</td>\n",
       "      <td>0.217407</td>\n",
       "      <td>-0.178868</td>\n",
       "      <td>0.565459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diabetes_binary</th>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.011030</td>\n",
       "      <td>-0.002274</td>\n",
       "      <td>-0.004097</td>\n",
       "      <td>-0.006953</td>\n",
       "      <td>-0.002408</td>\n",
       "      <td>-0.004789</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>-0.001889</td>\n",
       "      <td>-0.001104</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>-0.005024</td>\n",
       "      <td>-0.011795</td>\n",
       "      <td>-0.004940</td>\n",
       "      <td>-0.012193</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>-0.003078</td>\n",
       "      <td>0.004480</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        HighBP  HighChol  CholCheck       BMI    Smoker  \\\n",
       "HighBP                1.000000  0.290281   0.019190  0.252404  0.132512   \n",
       "HighChol              0.290281  1.000000   0.012837  0.129130  0.125240   \n",
       "CholCheck             0.019190  0.012837   1.000000 -0.004101 -0.005190   \n",
       "BMI                   0.252404  0.129130  -0.004101  1.000000  0.063508   \n",
       "Smoker                0.132512  0.125240  -0.005190  0.063508  1.000000   \n",
       "Stroke                0.119859  0.081647  -0.000986  0.057133  0.072942   \n",
       "HeartDiseaseorAttack  0.192153  0.160192  -0.003985  0.098065  0.144389   \n",
       "PhysActivity         -0.185341 -0.123335   0.019316 -0.255187 -0.102905   \n",
       "Fruits               -0.103985 -0.089436   0.015673 -0.158303 -0.102695   \n",
       "Veggies              -0.123597 -0.084961   0.010999 -0.123689 -0.060063   \n",
       "HvyAlcoholConsump     0.022629  0.028740  -0.005638  0.000244  0.066169   \n",
       "AnyHealthcare        -0.032929 -0.023433   0.079857 -0.064963 -0.028095   \n",
       "NoDocbcCost           0.077367  0.065678  -0.059807  0.129655  0.042161   \n",
       "GenHlth               0.322079  0.223396  -0.020087  0.345232  0.181521   \n",
       "MentHlth              0.124688  0.117957  -0.027875  0.201497  0.108281   \n",
       "PhysHlth              0.188203  0.139995  -0.014818  0.255436  0.140699   \n",
       "DiffWalk              0.225602  0.147227  -0.009001  0.316306  0.140045   \n",
       "Sex                  -0.011667 -0.007690  -0.010735 -0.044020  0.115278   \n",
       "Age                   0.275838  0.163192   0.022321 -0.112717  0.145365   \n",
       "Education            -0.227568 -0.134820   0.006402 -0.201685 -0.171960   \n",
       "Income               -0.282530 -0.163382   0.031126 -0.242094 -0.152527   \n",
       "Diabetes_binary      -0.012546 -0.011030  -0.002274 -0.004097 -0.006953   \n",
       "\n",
       "                        Stroke  HeartDiseaseorAttack  PhysActivity    Fruits  \\\n",
       "HighBP                0.119859              0.192153     -0.185341 -0.103985   \n",
       "HighChol              0.081647              0.160192     -0.123335 -0.089436   \n",
       "CholCheck            -0.000986             -0.003985      0.019316  0.015673   \n",
       "BMI                   0.057133              0.098065     -0.255187 -0.158303   \n",
       "Smoker                0.072942              0.144389     -0.102905 -0.102695   \n",
       "Stroke                1.000000              0.233298     -0.126957 -0.046131   \n",
       "HeartDiseaseorAttack  0.233298              1.000000     -0.147135 -0.063526   \n",
       "PhysActivity         -0.126957             -0.147135      1.000000  0.165934   \n",
       "Fruits               -0.046131             -0.063526      0.165934  1.000000   \n",
       "Veggies              -0.084637             -0.085198      0.204442  0.245612   \n",
       "HvyAlcoholConsump    -0.008613             -0.008301     -0.010663 -0.027089   \n",
       "AnyHealthcare        -0.028131             -0.021283      0.072273  0.046059   \n",
       "NoDocbcCost           0.077127              0.083611     -0.120715 -0.070790   \n",
       "GenHlth               0.223366              0.311562     -0.372282 -0.189348   \n",
       "MentHlth              0.141691              0.140186     -0.224465 -0.103770   \n",
       "PhysHlth              0.210303              0.246269     -0.333044 -0.113491   \n",
       "DiffWalk              0.236295              0.271997     -0.363598 -0.116434   \n",
       "Sex                  -0.019964              0.073961      0.081863 -0.096090   \n",
       "Age                   0.083517              0.173327     -0.080338  0.042817   \n",
       "Education            -0.120345             -0.153377      0.273416  0.145128   \n",
       "Income               -0.194099             -0.216515      0.307727  0.140828   \n",
       "Diabetes_binary      -0.002408             -0.004789      0.002891  0.006887   \n",
       "\n",
       "                       Veggies  HvyAlcoholConsump  AnyHealthcare  NoDocbcCost  \\\n",
       "HighBP               -0.123597           0.022629      -0.032929     0.077367   \n",
       "HighChol             -0.084961           0.028740      -0.023433     0.065678   \n",
       "CholCheck             0.010999          -0.005638       0.079857    -0.059807   \n",
       "BMI                  -0.123689           0.000244      -0.064963     0.129655   \n",
       "Smoker               -0.060063           0.066169      -0.028095     0.042161   \n",
       "Stroke               -0.084637          -0.008613      -0.028131     0.077127   \n",
       "HeartDiseaseorAttack -0.085198          -0.008301      -0.021283     0.083611   \n",
       "PhysActivity          0.204442          -0.010663       0.072273    -0.120715   \n",
       "Fruits                0.245612          -0.027089       0.046059    -0.070790   \n",
       "Veggies               1.000000           0.002835       0.057291    -0.088002   \n",
       "HvyAlcoholConsump     0.002835           1.000000      -0.023685     0.012283   \n",
       "AnyHealthcare         0.057291          -0.023685       1.000000    -0.231933   \n",
       "NoDocbcCost          -0.088002           0.012283      -0.231933     1.000000   \n",
       "GenHlth              -0.214869           0.013480      -0.107130     0.231035   \n",
       "MentHlth             -0.119102           0.025015      -0.095317     0.242863   \n",
       "PhysHlth             -0.151267           0.004426      -0.068226     0.221550   \n",
       "DiffWalk             -0.165544          -0.003393      -0.053400     0.197706   \n",
       "Sex                  -0.027881           0.028414       0.021073    -0.062672   \n",
       "Age                  -0.029046          -0.021322       0.087374    -0.103265   \n",
       "Education             0.214979           0.007189       0.125511    -0.151419   \n",
       "Income                0.238881           0.020969       0.157447    -0.249087   \n",
       "Diabetes_binary      -0.001889          -0.001104       0.007229     0.004033   \n",
       "\n",
       "                       GenHlth  MentHlth  PhysHlth  DiffWalk       Sex  \\\n",
       "HighBP                0.322079  0.124688  0.188203  0.225602 -0.011667   \n",
       "HighChol              0.223396  0.117957  0.139995  0.147227 -0.007690   \n",
       "CholCheck            -0.020087 -0.027875 -0.014818 -0.009001 -0.010735   \n",
       "BMI                   0.345232  0.201497  0.255436  0.316306 -0.044020   \n",
       "Smoker                0.181521  0.108281  0.140699  0.140045  0.115278   \n",
       "Stroke                0.223366  0.141691  0.210303  0.236295 -0.019964   \n",
       "HeartDiseaseorAttack  0.311562  0.140186  0.246269  0.271997  0.073961   \n",
       "PhysActivity         -0.372282 -0.224465 -0.333044 -0.363598  0.081863   \n",
       "Fruits               -0.189348 -0.103770 -0.113491 -0.116434 -0.096090   \n",
       "Veggies              -0.214869 -0.119102 -0.151267 -0.165544 -0.027881   \n",
       "HvyAlcoholConsump     0.013480  0.025015  0.004426 -0.003393  0.028414   \n",
       "AnyHealthcare        -0.107130 -0.095317 -0.068226 -0.053400  0.021073   \n",
       "NoDocbcCost           0.231035  0.242863  0.221550  0.197706 -0.062672   \n",
       "GenHlth               1.000000  0.402433  0.587633  0.517395 -0.087672   \n",
       "MentHlth              0.402433  1.000000  0.480460  0.359089 -0.103016   \n",
       "PhysHlth              0.587633  0.480460  1.000000  0.551986 -0.079606   \n",
       "DiffWalk              0.517395  0.359089  0.551986  1.000000 -0.121975   \n",
       "Sex                  -0.087672 -0.103016 -0.079606 -0.121975  1.000000   \n",
       "Age                   0.056083 -0.089631  0.021680  0.112360 -0.014452   \n",
       "Education            -0.408064 -0.205666 -0.262703 -0.297588  0.117552   \n",
       "Income               -0.506399 -0.321325 -0.395411 -0.448740  0.217407   \n",
       "Diabetes_binary      -0.005024 -0.011795 -0.004940 -0.012193  0.012702   \n",
       "\n",
       "                           Age  Education    Income  Diabetes_binary  \n",
       "HighBP                0.275838  -0.227568 -0.282530        -0.012546  \n",
       "HighChol              0.163192  -0.134820 -0.163382        -0.011030  \n",
       "CholCheck             0.022321   0.006402  0.031126        -0.002274  \n",
       "BMI                  -0.112717  -0.201685 -0.242094        -0.004097  \n",
       "Smoker                0.145365  -0.171960 -0.152527        -0.006953  \n",
       "Stroke                0.083517  -0.120345 -0.194099        -0.002408  \n",
       "HeartDiseaseorAttack  0.173327  -0.153377 -0.216515        -0.004789  \n",
       "PhysActivity         -0.080338   0.273416  0.307727         0.002891  \n",
       "Fruits                0.042817   0.145128  0.140828         0.006887  \n",
       "Veggies              -0.029046   0.214979  0.238881        -0.001889  \n",
       "HvyAlcoholConsump    -0.021322   0.007189  0.020969        -0.001104  \n",
       "AnyHealthcare         0.087374   0.125511  0.157447         0.007229  \n",
       "NoDocbcCost          -0.103265  -0.151419 -0.249087         0.004033  \n",
       "GenHlth               0.056083  -0.408064 -0.506399        -0.005024  \n",
       "MentHlth             -0.089631  -0.205666 -0.321325        -0.011795  \n",
       "PhysHlth              0.021680  -0.262703 -0.395411        -0.004940  \n",
       "DiffWalk              0.112360  -0.297588 -0.448740        -0.012193  \n",
       "Sex                  -0.014452   0.117552  0.217407         0.012702  \n",
       "Age                   1.000000  -0.124945 -0.178868        -0.003078  \n",
       "Education            -0.124945   1.000000  0.565459         0.004480  \n",
       "Income               -0.178868   0.565459  1.000000         0.008735  \n",
       "Diabetes_binary      -0.003078   0.004480  0.008735         1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df_undersampled_train.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c784adf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HighBP             0.012546\n",
       "HighChol           0.011030\n",
       "Smoker             0.006953\n",
       "Fruits             0.006887\n",
       "AnyHealthcare      0.007229\n",
       "MentHlth           0.011795\n",
       "DiffWalk           0.012193\n",
       "Sex                0.012702\n",
       "Income             0.008735\n",
       "Diabetes_binary    1.000000\n",
       "Name: Diabetes_binary, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_target = abs(corr[\"Diabetes_binary\"])\n",
    "relevant_features = corr_target[corr_target>0.006]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c6798",
   "metadata": {},
   "source": [
    "# Creating new DF with selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e26a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49484 entries, 0 to 49483\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Sex            49484 non-null  float64\n",
      " 1   HighBP         49484 non-null  float64\n",
      " 2   DiffWalk       49484 non-null  float64\n",
      " 3   MentHlth       49484 non-null  float64\n",
      " 4   HighChol       49484 non-null  float64\n",
      " 5   AnyHealthcare  49484 non-null  float64\n",
      " 6   Smoker         49484 non-null  float64\n",
      " 7   Fruits         49484 non-null  float64\n",
      " 8   Income         49484 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 3.4 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21208 entries, 0 to 21207\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Sex            21208 non-null  float64\n",
      " 1   HighBP         21208 non-null  float64\n",
      " 2   DiffWalk       21208 non-null  float64\n",
      " 3   MentHlth       21208 non-null  float64\n",
      " 4   HighChol       21208 non-null  float64\n",
      " 5   AnyHealthcare  21208 non-null  float64\n",
      " 6   Smoker         21208 non-null  float64\n",
      " 7   Fruits         21208 non-null  float64\n",
      " 8   Income         21208 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X_selected_train = df_undersampled_train.loc[:, ['Sex', 'HighBP', 'DiffWalk', 'MentHlth', 'HighChol', 'AnyHealthcare', 'Smoker',\n",
    "                       'Fruits','Income']]\n",
    "print(X_selected_train.info())\n",
    "\n",
    "X_selected_test = df_undersampled_test.loc[:, ['Sex', 'HighBP', 'DiffWalk', 'MentHlth', 'HighChol', 'AnyHealthcare', 'Smoker',\n",
    "                       'Fruits','Income']]\n",
    "print(X_selected_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78350df7",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "08ef308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Perceptron(), n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 1, 2],\n",
       "                         'early_stopping': [True, False],\n",
       "                         'penalty': ['l2', 'l1', 'elasticnet', 'None'],\n",
       "                         'random_state': [42]},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_linear = GridSearchCV(estimator = Perceptron(),\n",
    "                       param_grid = {'penalty': ['l2', 'l1', 'elasticnet', 'None'],\n",
    "                                     'alpha': [0.0001, 0.001, 0.01, 1, 2],\n",
    "                                    'early_stopping': [True, False],\n",
    "                                    'random_state': [42]},\n",
    "                       cv = 5,\n",
    "                       scoring='accuracy',\n",
    "                        verbose = 3,\n",
    "                        n_jobs = -1)\n",
    "gs_linear.fit(X_selected_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "00aca1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'early_stopping': False, 'penalty': 'None', 'random_state': 42}\n",
      "0.7810608891632318\n"
     ]
    }
   ],
   "source": [
    "print(gs_linear.best_params_)\n",
    "print(gs_linear.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63470dde",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e9ad2be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 112 candidates, totalling 560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=-1,\n",
       "             param_grid={'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
       "                         'n_neighbors': range(3, 10), 'p': [1, 2],\n",
       "                         'weights': ['uniform', 'distance']},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(estimator=KNeighborsClassifier(),\n",
    "                 param_grid = {'n_neighbors': range(3, 10, 1),\n",
    "                              'weights': ['uniform', 'distance'],\n",
    "                              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                              'p': [1, 2],\n",
    "                              },\n",
    "                 cv=5,\n",
    "                 scoring='accuracy',\n",
    "                 verbose = 3,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_selected_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4fcf517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'ball_tree', 'n_neighbors': 6, 'p': 2, 'weights': 'distance'}\n",
      "0.8514267754149062\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb208b0",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d238e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "2000 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 281, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid value for max_features. Allowed string values are 'auto', 'sqrt' or 'log2'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "800 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.81202015 0.81202015 0.81202015 0.81204036 0.81199994 0.81199994\n",
      " 0.81199994 0.81202015 0.81204035 0.81204035 0.81204035 0.81204035\n",
      " 0.81204035 0.81204035 0.81204035 0.81204035 0.81202015 0.81202015\n",
      " 0.81202015 0.81204036 0.81199994 0.81199994 0.81199994 0.81202015\n",
      " 0.81204035 0.81204035 0.81204035 0.81204035 0.81204035 0.81204035\n",
      " 0.81204035 0.81204035        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81323261 0.81353573 0.81331345 0.81371761 0.8131922  0.8131922\n",
      " 0.8131922  0.81355595 0.81311136 0.81311136 0.81311136 0.81311136\n",
      " 0.81292948 0.81292948 0.81292948 0.81292948 0.81323261 0.81353573\n",
      " 0.81331345 0.81371761 0.8131922  0.8131922  0.8131922  0.81355595\n",
      " 0.81311136 0.81311136 0.81311136 0.81311136 0.81292948 0.81292948\n",
      " 0.81292948 0.81292948        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81193928 0.81232324 0.81232324 0.81224241 0.8129901  0.8129901\n",
      " 0.8129901  0.81278803 0.81266678 0.81266678 0.81266678 0.81266678\n",
      " 0.81254553 0.81254553 0.81254553 0.81254553 0.81193928 0.81232324\n",
      " 0.81232324 0.81224241 0.8129901  0.8129901  0.8129901  0.81278803\n",
      " 0.81266678 0.81266678 0.81266678 0.81266678 0.81254553 0.81254553\n",
      " 0.81254553 0.81254553        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81202012 0.81228282 0.81232324 0.81224241 0.8129901  0.8129901\n",
      " 0.8129901  0.81278803 0.81266678 0.81266678 0.81266678 0.81266678\n",
      " 0.81254553 0.81254553 0.81254553 0.81254553 0.81202012 0.81228282\n",
      " 0.81232324 0.81224241 0.8129901  0.8129901  0.8129901  0.81278803\n",
      " 0.81266678 0.81266678 0.81266678 0.81266678 0.81254553 0.81254553\n",
      " 0.81254553 0.81254553        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81202012 0.81228282 0.81232324 0.81224241 0.8129901  0.8129901\n",
      " 0.8129901  0.81278803 0.81266678 0.81266678 0.81266678 0.81266678\n",
      " 0.81254553 0.81254553 0.81254553 0.81254553 0.81202012 0.81228282\n",
      " 0.81232324 0.81224241 0.8129901  0.8129901  0.8129901  0.81278803\n",
      " 0.81266678 0.81266678 0.81266678 0.81266678 0.81254553 0.81254553\n",
      " 0.81254553 0.81254553        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81082782 0.81082782 0.81078741 0.81070657 0.81111074 0.81111074\n",
      " 0.81111074 0.8110299  0.8110299  0.8110299  0.8110299  0.8110299\n",
      " 0.81064594 0.81064594 0.81064594 0.81064594 0.81082782 0.81082782\n",
      " 0.81078741 0.81070657 0.81111074 0.81111074 0.81111074 0.8110299\n",
      " 0.8110299  0.8110299  0.8110299  0.8110299  0.81064594 0.81064594\n",
      " 0.81064594 0.81064594        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81331345 0.81359636 0.8129497  0.81311137 0.81315178 0.81315178\n",
      " 0.81315178 0.81335386 0.81299011 0.81299011 0.81299011 0.81299011\n",
      " 0.81288908 0.81288908 0.81288908 0.81288908 0.81331345 0.81359636\n",
      " 0.8129497  0.81311137 0.81315178 0.81315178 0.81315178 0.81335386\n",
      " 0.81299011 0.81299011 0.81299011 0.81299011 0.81288908 0.81288908\n",
      " 0.81288908 0.81288908        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8122424  0.81222219 0.81210095 0.81218179 0.81264656 0.81264656\n",
      " 0.81264656 0.81294968 0.81305074 0.81305074 0.81305074 0.81305074\n",
      " 0.81228282 0.81228282 0.81228282 0.81228282 0.8122424  0.81222219\n",
      " 0.81210095 0.81218179 0.81264656 0.81264656 0.81264656 0.81294968\n",
      " 0.81305074 0.81305074 0.81305074 0.81305074 0.81228282 0.81228282\n",
      " 0.81228282 0.81228282        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81228282 0.81220198 0.81210095 0.81218179 0.81264656 0.81264656\n",
      " 0.81264656 0.81294968 0.81305074 0.81305074 0.81305074 0.81305074\n",
      " 0.81228282 0.81228282 0.81228282 0.81228282 0.81228282 0.81220198\n",
      " 0.81210095 0.81218179 0.81264656 0.81264656 0.81264656 0.81294968\n",
      " 0.81305074 0.81305074 0.81305074 0.81305074 0.81228282 0.81228282\n",
      " 0.81228282 0.81228282        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81228282 0.81220198 0.81210095 0.81218179 0.81264656 0.81264656\n",
      " 0.81264656 0.81294968 0.81305074 0.81305074 0.81305074 0.81305074\n",
      " 0.81228282 0.81228282 0.81228282 0.81228282 0.81228282 0.81220198\n",
      " 0.81210095 0.81218179 0.81264656 0.81264656 0.81264656 0.81294968\n",
      " 0.81305074 0.81305074 0.81305074 0.81305074 0.81228282 0.81228282\n",
      " 0.81228282 0.81228282        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_depth': range(5, 30, 5),\n",
       "                         'max_features': ['sqrt', 'log2', 'None'],\n",
       "                         'min_samples_leaf': range(2, 22, 5),\n",
       "                         'min_samples_split': range(2, 22, 5),\n",
       "                         'random_state': [42]},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_gs = GridSearchCV(estimator=RandomForestClassifier(),\n",
    "                 param_grid = {'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "                              'min_samples_split': range(2, 22, 5),\n",
    "                              'min_samples_leaf': range(2, 22, 5),\n",
    "                              'max_features': ['sqrt', 'log2', 'None'],\n",
    "                              'random_state': [42],\n",
    "                              'max_depth': range(5, 30, 5)},\n",
    "                 cv=5,\n",
    "                 scoring='accuracy',\n",
    "                 verbose = 3,\n",
    "                 n_jobs = -1)\n",
    "random_gs.fit(X_selected_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce66f53",
   "metadata": {},
   "source": [
    "rf = RandomForestClassifier()\n",
    "random_gs = GridSearchCV(estimator = rf,\n",
    "                 param_grid= {'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "                             'max_depth': range(5, 30, 5),\n",
    "                             'min_samples_split': range(2, 20, 2),\n",
    "                             'random_state': [42]},\n",
    "                 cv = 5,\n",
    "                 scoring='accuracy',\n",
    "                        verbose = 3,\n",
    "                        n_jobs = -1)\n",
    "random_gs.fit(X_selected_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9a65cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 17, 'random_state': 42}\n",
      "0.813717613161726\n"
     ]
    }
   ],
   "source": [
    "print(random_gs.best_params_)\n",
    "print(random_gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316652e",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3147ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 5, 10, 15, 20], 'degree': [3, 6, 9],\n",
       "                         'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'poly', 'rbf']},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_gs = GridSearchCV(estimator = SVC(),\n",
    "                 param_grid = {'C': [0.1, 1, 5, 10, 15, 20],\n",
    "                              'kernel': ['linear', 'poly', 'rbf'],\n",
    "                              'degree': [3, 6, 9],\n",
    "                              'gamma': ['scale', 'auto']},\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                     verbose = 3,\n",
    "                     n_jobs = -1)\n",
    "svc_gs.fit(X_selected_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8d5e5465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.8129496648952277\n"
     ]
    }
   ],
   "source": [
    "print(svc_gs.best_params_)\n",
    "print(svc_gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38fbdb",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2a621949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "765 fits failed out of a total of 1080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 434, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only solvers in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], got newton-cholesky.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "225 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.81204035        nan        nan        nan 0.81206056\n",
      " 0.81218181 0.81212119 0.81218181        nan 0.81218181 0.81218181\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81206056        nan        nan        nan 0.81197973\n",
      " 0.8121414  0.8121414  0.8121414         nan 0.81212119 0.8121414\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81206056        nan        nan        nan 0.81197973\n",
      " 0.81222223 0.81222223 0.81222223        nan 0.81222223 0.81222223\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81206056        nan        nan        nan 0.81197973\n",
      " 0.8121616  0.81220202 0.8121616         nan 0.8121616  0.81220202\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81206056        nan        nan        nan 0.81197973\n",
      " 0.81206056 0.81206056 0.81206056        nan 0.81204035 0.81204035\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81206056        nan        nan        nan 0.81197973\n",
      " 0.81206056 0.81206056 0.81206056        nan 0.81206056 0.81206056\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81206056        nan        nan        nan 0.81197973\n",
      " 0.81206056 0.81206056 0.81206056        nan 0.81206056 0.81206056\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81206056        nan        nan        nan 0.81197973\n",
      " 0.81206056 0.81206056 0.81206056        nan 0.81206056 0.81206056\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81206056        nan        nan        nan 0.81197973\n",
      " 0.81206056 0.81206056 0.81206056        nan 0.81206056 0.81206056\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': range(1, 10),\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet', 'None'],\n",
       "                         'random_state': [42],\n",
       "                         'solver': ['lbfgs', 'liblinear', 'newton-cg',\n",
       "                                    'newton-cholesky', 'sag', 'saga']},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_gs = GridSearchCV(estimator = LogisticRegression(),\n",
    "                     param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'None'],\n",
    "                                  'C': range(1, 10, 1),\n",
    "                                  'random_state': [42],\n",
    "                                  'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']},\n",
    "                     cv=5,\n",
    "                     scoring = 'accuracy',\n",
    "                     verbose = 3,\n",
    "                     n_jobs = -1)\n",
    "log_gs.fit(X_selected_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4642949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear'}\n",
      "0.812222230934976\n"
     ]
    }
   ],
   "source": [
    "print(log_gs.best_params_)\n",
    "print(log_gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9df276",
   "metadata": {},
   "source": [
    "# Testing For Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a8a36953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Train Accuracy: 0.793 & Test Accuracy: 0.79\n",
      "Random Forest Tree Train Accuracy: 0.816 & Test Accuracy: 0.816\n",
      "Linear Classifier/Perceptron Train Accuracy: 0.695 & Test Accuracy: 0.697\n",
      "SVM Train Accuracy: 0.814 & Test Accuracy: 0.816\n",
      "Logistic Regression Train Accuracy: 0.812 & Test Accuracy: 0.813\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(algorithm = 'ball_tree', n_neighbors = 9, p = 2, weights = 'uniform')\n",
    "knn.fit(X_selected_train, y_train.values.ravel())\n",
    "knn_train_pred = knn.predict(X_selected_train)\n",
    "knn_train_score = accuracy_score(y_train, knn_train_pred)\n",
    "knn_test_pred = knn.predict(X_selected_test)\n",
    "knn_test_score = accuracy_score(y_test, knn_test_pred)\n",
    "\n",
    "print(f'KNN Train Accuracy: {np.round(knn_train_score, 3)} & Test Accuracy: {np.round(knn_test_score, 3)}')\n",
    "\n",
    "rf = RandomForestClassifier(criterion = 'gini', max_depth = 10, min_samples_split = 18, random_state=42)\n",
    "rf.fit(X_selected_train, y_train.values.ravel())\n",
    "rf_train_pred = rf.predict(X_selected_train)\n",
    "rf_train_score = accuracy_score(y_train, rf_train_pred)\n",
    "rf_test_pred = rf.predict(X_selected_test)\n",
    "rf_test_score = accuracy_score(y_test, rf_test_pred)\n",
    "\n",
    "print(f'Random Forest Tree Train Accuracy: {np.round(rf_train_score, 3)} & Test Accuracy: {np.round(rf_test_score, 3)}')\n",
    "\n",
    "line = Perceptron(alpha = 0.0001, early_stopping = False, penalty = None, random_state=42)\n",
    "line.fit(X_selected_train, y_train.values.ravel())\n",
    "line_train_pred = line.predict(X_selected_train)\n",
    "line_train_score = accuracy_score(y_train, line_train_pred)\n",
    "line_test_pred = line.predict(X_selected_test)\n",
    "line_test_score = accuracy_score(y_test, line_test_pred)\n",
    "\n",
    "print(f'Linear Classifier/Perceptron Train Accuracy: {np.round(line_train_score, 3)} & Test Accuracy: {np.round(line_test_score, 3)}')\n",
    "\n",
    "svm = SVC(C = 5, degree = 3, gamma = 'scale', kernel = 'rbf')\n",
    "svm.fit(X_selected_train, y_train.values.ravel())\n",
    "svm_train_pred = svm.predict(X_selected_train)\n",
    "svm_train_score = accuracy_score(y_train, svm_train_pred)\n",
    "svm_test_pred = svm.predict(X_selected_test)\n",
    "svm_test_score = accuracy_score(y_test, svm_test_pred)\n",
    "\n",
    "print(f'SVM Train Accuracy: {np.round(svm_train_score, 3)} & Test Accuracy: {np.round(svm_test_score, 3)}')\n",
    "\n",
    "log = LogisticRegression(C = 1, penalty = 'l1', random_state=42, solver = 'saga')\n",
    "log.fit(X_selected_train, y_train.values.ravel())\n",
    "log_train_pred = log.predict(X_selected_train)\n",
    "log_train_score = accuracy_score(y_train, log_train_pred)\n",
    "log_test_pred = log.predict(X_selected_test)\n",
    "log_test_score = accuracy_score(y_test, log_test_pred)\n",
    "\n",
    "print(f'Logistic Regression Train Accuracy: {np.round(log_train_score, 3)} & Test Accuracy: {np.round(log_test_score, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "87f40e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_conf_train = confusion_matrix(y_train, knn_train_pred)\n",
    "knn_conf_test = confusion_matrix(y_test, knn_test_pred)\n",
    "\n",
    "log_conf_train = confusion_matrix(y_train, log_train_pred)\n",
    "log_conf_test = confusion_matrix(y_test, log_test_pred)\n",
    "\n",
    "svm_conf_train = confusion_matrix(y_train, svm_train_pred)\n",
    "svm_conf_test = confusion_matrix(y_test, svm_test_pred)\n",
    "\n",
    "line_conf_train = confusion_matrix(y_train, line_train_pred)\n",
    "line_conf_test = confusion_matrix(y_test, line_test_pred)\n",
    "\n",
    "rf_conf_train = confusion_matrix(y_train, rf_train_pred)\n",
    "rf_conf_test = confusion_matrix(y_test, rf_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3235a3a",
   "metadata": {},
   "source": [
    "# KNN Accuracy and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "32cf4390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Train Accuracy: 0.793 & Test Accuracy: 0.79\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[20488  4257]\n",
      " [ 5980 18759]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[8692 1909]\n",
      " [2548 8059]]\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.83      0.80     24745\n",
      "         1.0       0.82      0.76      0.79     24739\n",
      "\n",
      "    accuracy                           0.79     49484\n",
      "   macro avg       0.79      0.79      0.79     49484\n",
      "weighted avg       0.79      0.79      0.79     49484\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.82      0.80     10601\n",
      "         1.0       0.81      0.76      0.78     10607\n",
      "\n",
      "    accuracy                           0.79     21208\n",
      "   macro avg       0.79      0.79      0.79     21208\n",
      "weighted avg       0.79      0.79      0.79     21208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'KNN Train Accuracy: {np.round(knn_train_score, 3)} & Test Accuracy: {np.round(knn_test_score, 3)}\\n')\n",
    "\n",
    "print(f'Train Confusion Matrix:\\n{knn_conf_train}\\n\\nTest Confusion Matrix:\\n{knn_conf_test}\\n')\n",
    "print(f'Train Classification Report:\\n{classification_report(y_train, knn_train_pred)}\\nTest Classification Report:\\n{classification_report(y_test, knn_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd07cd7",
   "metadata": {},
   "source": [
    "# Logistic Regression Accuracy and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1abe00eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitc Regression Train Accuracy: 0.812 & Test Accuracy: 0.813\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[21878  2867]\n",
      " [ 6428 18311]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[9359 1242]\n",
      " [2729 7878]]\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.88      0.82     24745\n",
      "         1.0       0.86      0.74      0.80     24739\n",
      "\n",
      "    accuracy                           0.81     49484\n",
      "   macro avg       0.82      0.81      0.81     49484\n",
      "weighted avg       0.82      0.81      0.81     49484\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.88      0.82     10601\n",
      "         1.0       0.86      0.74      0.80     10607\n",
      "\n",
      "    accuracy                           0.81     21208\n",
      "   macro avg       0.82      0.81      0.81     21208\n",
      "weighted avg       0.82      0.81      0.81     21208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Logisitc Regression Train Accuracy: {np.round(log_train_score, 3)} & Test Accuracy: {np.round(log_test_score, 3)}\\n')\n",
    "\n",
    "print(f'Train Confusion Matrix:\\n{log_conf_train}\\n\\nTest Confusion Matrix:\\n{log_conf_test}\\n')\n",
    "print(f'Train Classification Report:\\n{classification_report(y_train, log_train_pred)}\\nTest Classification Report:\\n{classification_report(y_test, log_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7462c08",
   "metadata": {},
   "source": [
    "# SVM Accuracy and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "47628118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Train Accuracy: 0.814 & Test Accuracy: 0.815\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[22382  2363]\n",
      " [ 6839 17900]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[9592 1009]\n",
      " [2917 7690]]\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.90      0.83     24745\n",
      "         1.0       0.88      0.72      0.80     24739\n",
      "\n",
      "    accuracy                           0.81     49484\n",
      "   macro avg       0.82      0.81      0.81     49484\n",
      "weighted avg       0.82      0.81      0.81     49484\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.90      0.83     10601\n",
      "         1.0       0.88      0.72      0.80     10607\n",
      "\n",
      "    accuracy                           0.81     21208\n",
      "   macro avg       0.83      0.81      0.81     21208\n",
      "weighted avg       0.83      0.81      0.81     21208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'SVM Train Accuracy: {np.round(svm_train_score, 3)} & Test Accuracy: {np.round(svm_test_score, 3)}\\n')\n",
    "\n",
    "print(f'Train Confusion Matrix:\\n{svm_conf_train}\\n\\nTest Confusion Matrix:\\n{svm_conf_test}\\n')\n",
    "print(f'Train Classification Report:\\n{classification_report(y_train, svm_train_pred)}\\nTest Classification Report:\\n{classification_report(y_test, svm_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e2a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8205c3b",
   "metadata": {},
   "source": [
    "# Perceptron Accuracy and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6287ac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Train Accuracy: 0.695 & Test Accuracy: 0.697\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[16250  8495]\n",
      " [ 6616 18123]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[6961 3640]\n",
      " [2777 7830]]\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.66      0.68     24745\n",
      "         1.0       0.68      0.73      0.71     24739\n",
      "\n",
      "    accuracy                           0.69     49484\n",
      "   macro avg       0.70      0.69      0.69     49484\n",
      "weighted avg       0.70      0.69      0.69     49484\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.66      0.68     10601\n",
      "         1.0       0.68      0.74      0.71     10607\n",
      "\n",
      "    accuracy                           0.70     21208\n",
      "   macro avg       0.70      0.70      0.70     21208\n",
      "weighted avg       0.70      0.70      0.70     21208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Perceptron Train Accuracy: {np.round(line_train_score, 3)} & Test Accuracy: {np.round(line_test_score, 3)}\\n')\n",
    "\n",
    "print(f'Train Confusion Matrix:\\n{line_conf_train}\\n\\nTest Confusion Matrix:\\n{line_conf_test}\\n')\n",
    "print(f'Train Classification Report:\\n{classification_report(y_train, line_train_pred)}\\nTest Classification Report:\\n{classification_report(y_test, line_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd47edc",
   "metadata": {},
   "source": [
    "# Random Forest Tree Accuracy and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f93bc3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Accuracy: 0.816 & Test Accuracy: 0.816\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[22210  2535]\n",
      " [ 6556 18183]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[9493 1108]\n",
      " [2791 7816]]\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.90      0.83     24745\n",
      "         1.0       0.88      0.73      0.80     24739\n",
      "\n",
      "    accuracy                           0.82     49484\n",
      "   macro avg       0.82      0.82      0.82     49484\n",
      "weighted avg       0.82      0.82      0.82     49484\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.90      0.83     10601\n",
      "         1.0       0.88      0.74      0.80     10607\n",
      "\n",
      "    accuracy                           0.82     21208\n",
      "   macro avg       0.82      0.82      0.81     21208\n",
      "weighted avg       0.82      0.82      0.81     21208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forest Train Accuracy: {np.round(rf_train_score, 3)} & Test Accuracy: {np.round(rf_test_score, 3)}\\n')\n",
    "\n",
    "print(f'Train Confusion Matrix:\\n{rf_conf_train}\\n\\nTest Confusion Matrix:\\n{rf_conf_test}\\n')\n",
    "print(f'Train Classification Report:\\n{classification_report(y_train, rf_train_pred)}\\nTest Classification Report:\\n{classification_report(y_test, rf_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbf079",
   "metadata": {},
   "source": [
    "# The Best Performing Model\n",
    "\n",
    "    * The Best Performing Model is Random Forest\n",
    "    \n",
    "    * The 3 best performing models are Random Forest, SVM, and Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09a20ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialFeatureSelector(direction='backward',\n",
       "                          estimator=KNeighborsClassifier(n_neighbors=3),\n",
       "                          n_features_to_select=10, n_jobs=-1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs_forward = SequentialFeatureSelector(KNeighborsClassifier(n_neighbors=3), n_features_to_select=10, n_jobs=-1)\n",
    "sfs_forward.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "\n",
    "sfs_backward = SequentialFeatureSelector(KNeighborsClassifier(n_neighbors=3), n_features_to_select=10,direction = 'backward', n_jobs=-1)\n",
    "sfs_backward.fit(X_train_scaled, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0e7440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HeartDiseaseorAttack  PhysActivity    Fruits  HvyAlcoholConsump  \\\n",
      "0                 -0.384172      0.514775  0.669234          -0.139079   \n",
      "1                  2.603001     -1.942597  0.669234          -0.139079   \n",
      "2                 -0.384172      0.514775 -1.494245          -0.139079   \n",
      "3                 -0.384172      0.514775  0.669234          -0.139079   \n",
      "4                 -0.384172     -1.942597  0.669234          -0.139079   \n",
      "...                     ...           ...       ...                ...   \n",
      "49479             -0.384172     -1.942597  0.669234          -0.139079   \n",
      "49480             -0.384172      0.514775 -1.494245          -0.139079   \n",
      "49481             -0.384172      0.514775  0.669234          -0.139079   \n",
      "49482             -0.384172      0.514775  0.669234          -0.139079   \n",
      "49483             -0.384172      0.514775  0.669234          -0.139079   \n",
      "\n",
      "       AnyHealthcare   GenHlth  MentHlth  PhysHlth  DiffWalk       Sex  \n",
      "0           0.147309 -0.639948 -0.332699 -0.446002 -0.485309 -1.036097  \n",
      "1           0.147309  1.255723 -0.332699 -0.446002  2.060543 -1.036097  \n",
      "2           0.147309 -0.639948 -0.332699 -0.446002 -0.485309  0.965161  \n",
      "3           0.147309 -1.587783 -0.332699 -0.446002 -0.485309 -1.036097  \n",
      "4           0.147309 -0.639948  2.664163  0.116531 -0.485309 -1.036097  \n",
      "...              ...       ...       ...       ...       ...       ...  \n",
      "49479       0.147309  0.307887 -0.332699 -0.446002 -0.485309  0.965161  \n",
      "49480       0.147309 -0.639948 -0.332699 -0.446002 -0.485309  0.965161  \n",
      "49481       0.147309 -0.639948 -0.332699 -0.220989 -0.485309 -1.036097  \n",
      "49482       0.147309 -0.639948 -0.332699 -0.446002 -0.485309  0.965161  \n",
      "49483       0.147309 -0.639948 -0.332699 -0.446002 -0.485309  0.965161  \n",
      "\n",
      "[49484 rows x 10 columns]\n",
      "[ 6  7  8 10 11 13 14 15 16 17]\n",
      "            BMI  PhysActivity   Veggies  AnyHealthcare   GenHlth  MentHlth  \\\n",
      "0     -1.221972      0.514775  0.418265       0.147309 -0.639948 -0.332699   \n",
      "1      0.697964     -1.942597 -2.390830       0.147309  1.255723 -0.332699   \n",
      "2     -1.061978      0.514775  0.418265       0.147309 -0.639948 -0.332699   \n",
      "3     -0.581994      0.514775  0.418265       0.147309 -1.587783 -0.332699   \n",
      "4      1.177948     -1.942597 -2.390830       0.147309 -0.639948  2.664163   \n",
      "...         ...           ...       ...            ...       ...       ...   \n",
      "49479 -0.262004     -1.942597  0.418265       0.147309  0.307887 -0.332699   \n",
      "49480 -0.262004      0.514775  0.418265       0.147309 -0.639948 -0.332699   \n",
      "49481 -1.061978      0.514775  0.418265       0.147309 -0.639948 -0.332699   \n",
      "49482  0.057985      0.514775  0.418265       0.147309 -0.639948 -0.332699   \n",
      "49483  0.857959      0.514775  0.418265       0.147309 -0.639948 -0.332699   \n",
      "\n",
      "       PhysHlth  DiffWalk       Sex    Income  \n",
      "0     -0.446002 -0.485309 -1.036097  0.356032  \n",
      "1     -0.446002  2.060543 -1.036097 -2.571665  \n",
      "2     -0.446002 -0.485309  0.965161  0.843982  \n",
      "3     -0.446002 -0.485309 -1.036097  0.843982  \n",
      "4      0.116531 -0.485309 -1.036097 -0.131917  \n",
      "...         ...       ...       ...       ...  \n",
      "49479 -0.446002 -0.485309  0.965161 -0.619867  \n",
      "49480 -0.446002 -0.485309  0.965161  0.843982  \n",
      "49481 -0.220989 -0.485309 -1.036097 -1.107816  \n",
      "49482 -0.446002 -0.485309  0.965161  0.843982  \n",
      "49483 -0.446002 -0.485309  0.965161  0.843982  \n",
      "\n",
      "[49484 rows x 10 columns]\n",
      "[ 3  7  9 11 13 14 15 16 17 20]\n"
     ]
    }
   ],
   "source": [
    "cols_idxs_forward = sfs_forward.get_support(indices=True)\n",
    "cols_idxs_forward\n",
    "\n",
    "forward_selection_train = df_undersampled_train.iloc[:, cols_idxs_forward]\n",
    "print(forward_selection_train)\n",
    "print(cols_idxs_forward)\n",
    "\n",
    "cols_idxs_backward = sfs_backward.get_support(indices=True)\n",
    "cols_idxs_backward\n",
    "\n",
    "backward_selection_train = df_undersampled_train.iloc[:, cols_idxs_backward]\n",
    "print(backward_selection_train)\n",
    "print(cols_idxs_backward)\n",
    "\n",
    "forward_selection_test = df_undersampled_test.iloc[:, cols_idxs_forward]\n",
    "backward_selection_test = df_undersampled_test.iloc[:, cols_idxs_backward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a265eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BMI    Stroke  PhysActivity   Veggies  AnyHealthcare   GenHlth  \\\n",
      "0     -1.221972 -0.225623      0.514775  0.418265       0.147309 -0.639948   \n",
      "1      0.697964 -0.225623     -1.942597 -2.390830       0.147309  1.255723   \n",
      "2     -1.061978 -0.225623      0.514775  0.418265       0.147309 -0.639948   \n",
      "3     -0.581994 -0.225623      0.514775  0.418265       0.147309 -1.587783   \n",
      "4      1.177948 -0.225623     -1.942597 -2.390830       0.147309 -0.639948   \n",
      "...         ...       ...           ...       ...            ...       ...   \n",
      "49479 -0.262004 -0.225623     -1.942597  0.418265       0.147309  0.307887   \n",
      "49480 -0.262004 -0.225623      0.514775  0.418265       0.147309 -0.639948   \n",
      "49481 -1.061978  4.432171      0.514775  0.418265       0.147309 -0.639948   \n",
      "49482  0.057985 -0.225623      0.514775  0.418265       0.147309 -0.639948   \n",
      "49483  0.857959 -0.225623      0.514775  0.418265       0.147309 -0.639948   \n",
      "\n",
      "       MentHlth  PhysHlth  DiffWalk    Income  \n",
      "0     -0.332699 -0.446002 -0.485309  0.356032  \n",
      "1     -0.332699 -0.446002  2.060543 -2.571665  \n",
      "2     -0.332699 -0.446002 -0.485309  0.843982  \n",
      "3     -0.332699 -0.446002 -0.485309  0.843982  \n",
      "4      2.664163  0.116531 -0.485309 -0.131917  \n",
      "...         ...       ...       ...       ...  \n",
      "49479 -0.332699 -0.446002 -0.485309 -0.619867  \n",
      "49480 -0.332699 -0.446002 -0.485309  0.843982  \n",
      "49481 -0.332699 -0.220989 -0.485309 -1.107816  \n",
      "49482 -0.332699 -0.446002 -0.485309  0.843982  \n",
      "49483 -0.332699 -0.446002 -0.485309  0.843982  \n",
      "\n",
      "[49484 rows x 10 columns]\n",
      "[ 3  5  7  9 11 13 14 15 16 20]\n",
      "         Stroke  HeartDiseaseorAttack  PhysActivity  NoDocbcCost   GenHlth  \\\n",
      "0     -0.225623             -0.384172      0.514775    -0.237966 -0.639948   \n",
      "1     -0.225623              2.603001     -1.942597    -0.237966  1.255723   \n",
      "2     -0.225623             -0.384172      0.514775    -0.237966 -0.639948   \n",
      "3     -0.225623             -0.384172      0.514775    -0.237966 -1.587783   \n",
      "4     -0.225623             -0.384172     -1.942597    -0.237966 -0.639948   \n",
      "...         ...                   ...           ...          ...       ...   \n",
      "49479 -0.225623             -0.384172     -1.942597    -0.237966  0.307887   \n",
      "49480 -0.225623             -0.384172      0.514775    -0.237966 -0.639948   \n",
      "49481  4.432171             -0.384172      0.514775    -0.237966 -0.639948   \n",
      "49482 -0.225623             -0.384172      0.514775    -0.237966 -0.639948   \n",
      "49483 -0.225623             -0.384172      0.514775    -0.237966 -0.639948   \n",
      "\n",
      "       MentHlth  PhysHlth  DiffWalk  Education    Income  \n",
      "0     -0.332699 -0.446002 -0.485309   0.864409  0.356032  \n",
      "1     -0.332699 -0.446002  2.060543  -1.162526 -2.571665  \n",
      "2     -0.332699 -0.446002 -0.485309  -0.149059  0.843982  \n",
      "3     -0.332699 -0.446002 -0.485309  -0.149059  0.843982  \n",
      "4      2.664163  0.116531 -0.485309  -1.162526 -0.131917  \n",
      "...         ...       ...       ...        ...       ...  \n",
      "49479 -0.332699 -0.446002 -0.485309  -1.162526 -0.619867  \n",
      "49480 -0.332699 -0.446002 -0.485309   0.864409  0.843982  \n",
      "49481 -0.332699 -0.220989 -0.485309   0.864409 -1.107816  \n",
      "49482 -0.332699 -0.446002 -0.485309   0.864409  0.843982  \n",
      "49483 -0.332699 -0.446002 -0.485309   0.864409  0.843982  \n",
      "\n",
      "[49484 rows x 10 columns]\n",
      "[ 5  6  7 12 13 14 15 16 19 20]\n"
     ]
    }
   ],
   "source": [
    "sfs_forward_rf = SequentialFeatureSelector(RandomForestClassifier(), n_features_to_select=10, n_jobs=-1)\n",
    "sfs_forward_rf.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "\n",
    "sfs_backward_rf = SequentialFeatureSelector(RandomForestClassifier(), n_features_to_select=10,direction = 'backward', n_jobs=-1)\n",
    "sfs_backward_rf.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "cols_idxs_forward_rf = sfs_forward_rf.get_support(indices=True)\n",
    "cols_idxs_forward_rf\n",
    "\n",
    "forward_selection_train_rf = df_undersampled_train.iloc[:, cols_idxs_forward_rf]\n",
    "print(forward_selection_train_rf)\n",
    "print(cols_idxs_forward_rf)\n",
    "\n",
    "cols_idxs_backward_rf = sfs_backward_rf.get_support(indices=True)\n",
    "cols_idxs_backward_rf\n",
    "\n",
    "backward_selection_train_rf = df_undersampled_train.iloc[:, cols_idxs_backward_rf]\n",
    "print(backward_selection_train_rf)\n",
    "print(cols_idxs_backward_rf)\n",
    "\n",
    "forward_selection_test_rf = df_undersampled_test.iloc[:, cols_idxs_forward_rf]\n",
    "backward_selection_test_rf = df_undersampled_test.iloc[:, cols_idxs_backward_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e9e748f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_wrapper = RandomForestClassifier(criterion = 'gini', max_depth = 10, min_samples_split = 18, random_state=42)\n",
    "rf_wrapper.fit(forward_selection_train, y_train.values.ravel())\n",
    "\n",
    "rfw_train_pred = rf_wrapper.predict(forward_selection_train)\n",
    "rfw_train_score = accuracy_score(y_train, rfw_train_pred)\n",
    "rfw_test_pred = rf_wrapper.predict(forward_selection_test)\n",
    "rfw_test_score = accuracy_score(y_test, rfw_test_pred)\n",
    "\n",
    "rfw_conf_train = confusion_matrix(y_train.values.ravel(), rfw_train_pred)\n",
    "rfw_conf_test = confusion_matrix(y_test.values.ravel(), rfw_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "3f1add9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Accuracy: 0.857 & Test Accuracy: 0.855\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[23319  1426]\n",
      " [ 5634 19105]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[9946  655]\n",
      " [2413 8194]]\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.94      0.87     24745\n",
      "         1.0       0.93      0.77      0.84     24739\n",
      "\n",
      "    accuracy                           0.86     49484\n",
      "   macro avg       0.87      0.86      0.86     49484\n",
      "weighted avg       0.87      0.86      0.86     49484\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.94      0.87     10601\n",
      "         1.0       0.93      0.77      0.84     10607\n",
      "\n",
      "    accuracy                           0.86     21208\n",
      "   macro avg       0.87      0.86      0.85     21208\n",
      "weighted avg       0.87      0.86      0.85     21208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forest Train Accuracy: {np.round(rfw_train_score, 3)} & Test Accuracy: {np.round(rfw_test_score, 3)}\\n')\n",
    "\n",
    "print(f'Train Confusion Matrix:\\n{rfw_conf_train}\\n\\nTest Confusion Matrix:\\n{rfw_conf_test}\\n')\n",
    "print(f'Train Classification Report:\\n{classification_report(y_train, rfw_train_pred)}\\nTest Classification Report:\\n{classification_report(y_test, rfw_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "95e09120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Accuracy: 0.88 & Test Accuracy: 0.874\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[23386  1359]\n",
      " [ 4597 20142]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[9950  651]\n",
      " [2011 8596]]\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.95      0.89     24745\n",
      "         1.0       0.94      0.81      0.87     24739\n",
      "\n",
      "    accuracy                           0.88     49484\n",
      "   macro avg       0.89      0.88      0.88     49484\n",
      "weighted avg       0.89      0.88      0.88     49484\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88     10601\n",
      "         1.0       0.93      0.81      0.87     10607\n",
      "\n",
      "    accuracy                           0.87     21208\n",
      "   macro avg       0.88      0.87      0.87     21208\n",
      "weighted avg       0.88      0.87      0.87     21208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_wrapper = RandomForestClassifier(criterion = 'gini', max_depth = 10, min_samples_split = 18, random_state=42)\n",
    "rf_wrapper.fit(backward_selection_train, y_train.values.ravel())\n",
    "\n",
    "rfw_train_pred = rf_wrapper.predict(backward_selection_train)\n",
    "rfw_train_score = accuracy_score(y_train, rfw_train_pred)\n",
    "rfw_test_pred = rf_wrapper.predict(backward_selection_test)\n",
    "rfw_test_score = accuracy_score(y_test, rfw_test_pred)\n",
    "\n",
    "rfw_conf_train = confusion_matrix(y_train.values.ravel(), rfw_train_pred)\n",
    "rfw_conf_test = confusion_matrix(y_test.values.ravel(), rfw_test_pred)\n",
    "\n",
    "print(f'Random Forest Train Accuracy: {np.round(rfw_train_score, 3)} & Test Accuracy: {np.round(rfw_test_score, 3)}\\n')\n",
    "\n",
    "print(f'Train Confusion Matrix:\\n{rfw_conf_train}\\n\\nTest Confusion Matrix:\\n{rfw_conf_test}\\n')\n",
    "print(f'Train Classification Report:\\n{classification_report(y_train, rfw_train_pred)}\\nTest Classification Report:\\n{classification_report(y_test, rfw_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818ce12",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "96f0948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [02:38:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0750514818a16474a-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"subsamples\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.7, 1],\n",
       "                         'learning_rate': [0.3, 0.6, 0.9],\n",
       "                         'max_depth': [6, 12, 18],\n",
       "                         'min_child_weight': [1, 5, 10],\n",
       "                         'objective': ['binary:logistic', 'binary:logitraw',\n",
       "                                       'binary:hinge'],\n",
       "                         'seed': [42], 'subsamples': [0.5, 0.75, 1]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs = GridSearchCV(estimator = XGBClassifier(),\n",
    "                     param_grid={'objective': ['binary:logistic', 'binary:logitraw', 'binary:hinge'],\n",
    "                                'learning_rate':[0.3, 0.6, 0.9],\n",
    "                                'max_depth': [6, 12, 18],\n",
    "                                'min_child_weight': [1, 5, 10],\n",
    "                                'subsamples': [0.5, 0.75, 1],\n",
    "                                'colsample_bytree': [0.5, 0.7, 1],\n",
    "                                'seed':[42]},\n",
    "                     cv = 5,\n",
    "                     verbose = 2,\n",
    "                     n_jobs = -1)\n",
    "xgb_gs.fit(X_selected_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "77e104c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5, 'learning_rate': 0.3, 'max_depth': 6, 'min_child_weight': 1, 'objective': 'binary:hinge', 'seed': 42, 'subsamples': 0.5}\n",
      "0.8142430494072782\n"
     ]
    }
   ],
   "source": [
    "print(xgb_gs.best_params_)\n",
    "print(xgb_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "34381080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8146673672298117\n",
      "Classification Report for Train: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.91      0.83     24745\n",
      "         1.0       0.88      0.72      0.80     24739\n",
      "\n",
      "    accuracy                           0.81     49484\n",
      "   macro avg       0.83      0.81      0.81     49484\n",
      "weighted avg       0.83      0.81      0.81     49484\n",
      "\n",
      "\n",
      "Test Accuracy: 0.8159656733308186\n",
      "Classification Report for Test: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.91      0.83     10601\n",
      "         1.0       0.88      0.73      0.80     10607\n",
      "\n",
      "    accuracy                           0.82     21208\n",
      "   macro avg       0.83      0.82      0.81     21208\n",
      "weighted avg       0.83      0.82      0.81     21208\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [03:40:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0750514818a16474a-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"subsamples\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth=6, min_child_weight=1,\n",
    "                    objective='binary:hinge', seed=1, subsamples=0.5)\n",
    "xgb.fit(X_selected_train, y_train.values.ravel())\n",
    "\n",
    "xbg_train_pred = xgb.predict(X_selected_train)\n",
    "xbg_train_score = accuracy_score(y_train.values.ravel(), xbg_train_pred)\n",
    "\n",
    "xbg_test_pred = xgb.predict(X_selected_test)\n",
    "xbg_test_score = accuracy_score(y_test.values.ravel(), xbg_test_pred)\n",
    "\n",
    "print(f'Train Accuracy: {xbg_train_score}')\n",
    "print(f'Classification Report for Train: \\n{classification_report(y_train.values.ravel(), xbg_train_pred)}\\n')\n",
    "\n",
    "print(f'Test Accuracy: {xbg_test_score}')\n",
    "print(f'Classification Report for Test: \\n{classification_report(y_test.values.ravel(), xbg_test_pred)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5096e56",
   "metadata": {},
   "source": [
    "# Extreme Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ceb2615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000000\n",
      "Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "input_length = X_selected_train.shape[1]\n",
    "hidden_units = 1000\n",
    "\n",
    "win = np.random.normal(size = [input_length, hidden_units])\n",
    "\n",
    "def input_to_hidden(x):\n",
    "    a = np.dot(x, win)\n",
    "    a = np.maximum(a, 0, a)\n",
    "    return a\n",
    "\n",
    "x_h_v = input_to_hidden(X_selected_train)\n",
    "x_h_t = np.transpose(x_h_v)\n",
    "w_out = np.dot(np.linalg.inv(np.dot(x_h_t, x_h_v)), np.dot(x_h_t, y_train))\n",
    "\n",
    "def predict(x):\n",
    "    x = input_to_hidden(x)\n",
    "    y = np.dot(x, w_out)\n",
    "    return y\n",
    "\n",
    "extreme_pred = predict(X_selected_test)\n",
    "num_correct = 0\n",
    "total = extreme_pred.shape[0]\n",
    "for i in range(total):\n",
    "    predicted = np.argmax(extreme_pred[i])\n",
    "    test = np.argmax(y_test.values.ravel()[i])\n",
    "    num_correct = num_correct + (1 if predicted == test else 0)\n",
    "    \n",
    "print('Accuracy: {:f}'.format(num_correct/total))\n",
    "\n",
    "extreme_pred = predict(X_selected_train)\n",
    "num_correct = 0\n",
    "total = extreme_pred.shape[0]\n",
    "for i in range(total):\n",
    "    predicted = np.argmax(extreme_pred[i])\n",
    "    train = np.argmax(y_train.values.ravel()[i])\n",
    "    num_correct = num_correct + (1 if predicted == train else 0)\n",
    "    \n",
    "print('Accuracy: {:f}'.format(num_correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ff3b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000000\n",
      "Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "input_length = X_selected_train.shape[1]\n",
    "hidden_units = 1000\n",
    "\n",
    "win = np.random.normal(size = [input_length, hidden_units])\n",
    "\n",
    "def input_to_hidden(x):\n",
    "    a = np.dot(x, win)\n",
    "    a = np.maximum(a, 0, a)\n",
    "    return a\n",
    "\n",
    "x_h_v = input_to_hidden(X_selected_train)\n",
    "x_h_t = np.transpose(x_h_v)\n",
    "w_out = np.dot(np.linalg.inv(np.dot(x_h_t, x_h_v)), np.dot(x_h_t, y_train))\n",
    "\n",
    "def predict(x):\n",
    "    x = input_to_hidden(x)\n",
    "    y = np.dot(x, w_out)\n",
    "    return y\n",
    "\n",
    "extreme_pred = predict(X_selected_test)\n",
    "num_correct = 0\n",
    "total = extreme_pred.shape[0]\n",
    "for i in range(total):\n",
    "    predicted = np.argmax(extreme_pred[i])\n",
    "    test = np.argmax(y_test.values.ravel()[i])\n",
    "    num_correct = num_correct + (1 if predicted == test else 0)\n",
    "    \n",
    "print('Accuracy of test set: {:f}'.format(num_correct/total))\n",
    "\n",
    "extreme_pred = predict(X_selected_train)\n",
    "num_correct = 0\n",
    "total = extreme_pred.shape[0]\n",
    "for i in range(total):\n",
    "    predicted = np.argmax(extreme_pred[i])\n",
    "    train = np.argmax(y_train.values.ravel()[i])\n",
    "    num_correct = num_correct + (1 if predicted == train else 0)\n",
    "    \n",
    "print('Accuracy of train set: {:f}'.format(num_correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102da3d",
   "metadata": {},
   "source": [
    "# Basic Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "675e05d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.4086 - accuracy: 0.8048\n",
      "Epoch 2/50\n",
      "4949/4949 [==============================] - 3s 640us/step - loss: 0.3908 - accuracy: 0.8123\n",
      "Epoch 3/50\n",
      "4949/4949 [==============================] - 3s 646us/step - loss: 0.3885 - accuracy: 0.8134\n",
      "Epoch 4/50\n",
      "4949/4949 [==============================] - 3s 642us/step - loss: 0.3879 - accuracy: 0.8132\n",
      "Epoch 5/50\n",
      "4949/4949 [==============================] - 3s 638us/step - loss: 0.3871 - accuracy: 0.8137\n",
      "Epoch 6/50\n",
      "4949/4949 [==============================] - 3s 637us/step - loss: 0.3868 - accuracy: 0.8140\n",
      "Epoch 7/50\n",
      "4949/4949 [==============================] - 3s 637us/step - loss: 0.3867 - accuracy: 0.8135\n",
      "Epoch 8/50\n",
      "4949/4949 [==============================] - 3s 638us/step - loss: 0.3861 - accuracy: 0.8131\n",
      "Epoch 9/50\n",
      "4949/4949 [==============================] - 3s 636us/step - loss: 0.3862 - accuracy: 0.8141\n",
      "Epoch 10/50\n",
      "4949/4949 [==============================] - 3s 640us/step - loss: 0.3858 - accuracy: 0.8141\n",
      "Epoch 11/50\n",
      "4949/4949 [==============================] - 3s 636us/step - loss: 0.3856 - accuracy: 0.8128\n",
      "Epoch 12/50\n",
      "4949/4949 [==============================] - 3s 633us/step - loss: 0.3854 - accuracy: 0.8139\n",
      "Epoch 13/50\n",
      "4949/4949 [==============================] - 3s 631us/step - loss: 0.3852 - accuracy: 0.8142\n",
      "Epoch 14/50\n",
      "4949/4949 [==============================] - 3s 629us/step - loss: 0.3853 - accuracy: 0.8137\n",
      "Epoch 15/50\n",
      "4949/4949 [==============================] - 3s 629us/step - loss: 0.3852 - accuracy: 0.8142\n",
      "Epoch 16/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.3852 - accuracy: 0.8141\n",
      "Epoch 17/50\n",
      "4949/4949 [==============================] - 3s 634us/step - loss: 0.3849 - accuracy: 0.8150\n",
      "Epoch 18/50\n",
      "4949/4949 [==============================] - 3s 637us/step - loss: 0.3847 - accuracy: 0.8147\n",
      "Epoch 19/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.3848 - accuracy: 0.8144\n",
      "Epoch 20/50\n",
      "4949/4949 [==============================] - 3s 636us/step - loss: 0.3847 - accuracy: 0.8141\n",
      "Epoch 21/50\n",
      "4949/4949 [==============================] - 3s 634us/step - loss: 0.3846 - accuracy: 0.8137\n",
      "Epoch 22/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.3844 - accuracy: 0.8146\n",
      "Epoch 23/50\n",
      "4949/4949 [==============================] - 3s 627us/step - loss: 0.3844 - accuracy: 0.8148\n",
      "Epoch 24/50\n",
      "4949/4949 [==============================] - 3s 630us/step - loss: 0.3848 - accuracy: 0.8138\n",
      "Epoch 25/50\n",
      "4949/4949 [==============================] - 3s 630us/step - loss: 0.3845 - accuracy: 0.8135\n",
      "Epoch 26/50\n",
      "4949/4949 [==============================] - 3s 631us/step - loss: 0.3845 - accuracy: 0.8141\n",
      "Epoch 27/50\n",
      "4949/4949 [==============================] - 3s 636us/step - loss: 0.3844 - accuracy: 0.8154\n",
      "Epoch 28/50\n",
      "4949/4949 [==============================] - 3s 631us/step - loss: 0.3844 - accuracy: 0.8141\n",
      "Epoch 29/50\n",
      "4949/4949 [==============================] - 3s 627us/step - loss: 0.3843 - accuracy: 0.8139\n",
      "Epoch 30/50\n",
      "4949/4949 [==============================] - 3s 625us/step - loss: 0.3842 - accuracy: 0.8137\n",
      "Epoch 31/50\n",
      "4949/4949 [==============================] - 3s 636us/step - loss: 0.3841 - accuracy: 0.8144\n",
      "Epoch 32/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.3839 - accuracy: 0.8141\n",
      "Epoch 33/50\n",
      "4949/4949 [==============================] - 3s 630us/step - loss: 0.3839 - accuracy: 0.8146\n",
      "Epoch 34/50\n",
      "4949/4949 [==============================] - 3s 635us/step - loss: 0.3839 - accuracy: 0.8149\n",
      "Epoch 35/50\n",
      "4949/4949 [==============================] - 3s 636us/step - loss: 0.3840 - accuracy: 0.8141\n",
      "Epoch 36/50\n",
      "4949/4949 [==============================] - 3s 630us/step - loss: 0.3838 - accuracy: 0.8146\n",
      "Epoch 37/50\n",
      "4949/4949 [==============================] - 3s 634us/step - loss: 0.3838 - accuracy: 0.8140\n",
      "Epoch 38/50\n",
      "4949/4949 [==============================] - 3s 638us/step - loss: 0.3835 - accuracy: 0.8146\n",
      "Epoch 39/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.3837 - accuracy: 0.8140\n",
      "Epoch 40/50\n",
      "4949/4949 [==============================] - 3s 633us/step - loss: 0.3836 - accuracy: 0.8149\n",
      "Epoch 41/50\n",
      "4949/4949 [==============================] - 3s 631us/step - loss: 0.3835 - accuracy: 0.8146\n",
      "Epoch 42/50\n",
      "4949/4949 [==============================] - 3s 633us/step - loss: 0.3835 - accuracy: 0.8143\n",
      "Epoch 43/50\n",
      "4949/4949 [==============================] - 3s 630us/step - loss: 0.3834 - accuracy: 0.8147\n",
      "Epoch 44/50\n",
      "4949/4949 [==============================] - 3s 638us/step - loss: 0.3833 - accuracy: 0.8148\n",
      "Epoch 45/50\n",
      "4949/4949 [==============================] - 3s 636us/step - loss: 0.3834 - accuracy: 0.8149\n",
      "Epoch 46/50\n",
      "4949/4949 [==============================] - 3s 634us/step - loss: 0.3835 - accuracy: 0.8137\n",
      "Epoch 47/50\n",
      "4949/4949 [==============================] - 3s 637us/step - loss: 0.3833 - accuracy: 0.8141\n",
      "Epoch 48/50\n",
      "4949/4949 [==============================] - 3s 637us/step - loss: 0.3833 - accuracy: 0.8141\n",
      "Epoch 49/50\n",
      "4949/4949 [==============================] - 3s 638us/step - loss: 0.3833 - accuracy: 0.8149\n",
      "Epoch 50/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.3832 - accuracy: 0.8143\n",
      "1547/1547 [==============================] - 1s 528us/step - loss: 0.3830 - accuracy: 0.8153\n",
      "663/663 [==============================] - 0s 530us/step - loss: 0.3913 - accuracy: 0.8145\n",
      "Train Accuracy: 0.8152534365653992\n",
      "Test Accuracy: 0.8144568204879761\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(20, input_shape=(9,), activation='relu'))\n",
    "nn.add(Dense(10, activation = 'relu'))\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn.compile(loss = 'binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "nn.fit(X_selected_train, y_train.values.ravel(), epochs=50, batch_size=10)\n",
    "\n",
    "_, train_accuracy = nn.evaluate(X_selected_train, y_train)\n",
    "_2, test_accuracy = nn.evaluate(X_selected_test, y_test)\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8272748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4949/4949 [==============================] - 4s 635us/step - loss: 0.3288 - accuracy: 0.8550\n",
      "Epoch 2/50\n",
      "4949/4949 [==============================] - 3s 644us/step - loss: 0.2967 - accuracy: 0.8727\n",
      "Epoch 3/50\n",
      "4949/4949 [==============================] - 3s 638us/step - loss: 0.2922 - accuracy: 0.8750\n",
      "Epoch 4/50\n",
      "4949/4949 [==============================] - 3s 638us/step - loss: 0.2902 - accuracy: 0.8747\n",
      "Epoch 5/50\n",
      "4949/4949 [==============================] - 3s 623us/step - loss: 0.2888 - accuracy: 0.8754\n",
      "Epoch 6/50\n",
      "4949/4949 [==============================] - 3s 640us/step - loss: 0.2880 - accuracy: 0.8758\n",
      "Epoch 7/50\n",
      "4949/4949 [==============================] - 3s 643us/step - loss: 0.2869 - accuracy: 0.8764\n",
      "Epoch 8/50\n",
      "4949/4949 [==============================] - 3s 648us/step - loss: 0.2864 - accuracy: 0.8758\n",
      "Epoch 9/50\n",
      "4949/4949 [==============================] - 3s 637us/step - loss: 0.2860 - accuracy: 0.8762\n",
      "Epoch 10/50\n",
      "4949/4949 [==============================] - 3s 652us/step - loss: 0.2851 - accuracy: 0.8766\n",
      "Epoch 11/50\n",
      "4949/4949 [==============================] - 3s 644us/step - loss: 0.2849 - accuracy: 0.8769\n",
      "Epoch 12/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.2848 - accuracy: 0.8779\n",
      "Epoch 13/50\n",
      "4949/4949 [==============================] - 3s 646us/step - loss: 0.2841 - accuracy: 0.8778\n",
      "Epoch 14/50\n",
      "4949/4949 [==============================] - 3s 648us/step - loss: 0.2838 - accuracy: 0.8772\n",
      "Epoch 15/50\n",
      "4949/4949 [==============================] - 3s 648us/step - loss: 0.2836 - accuracy: 0.8775\n",
      "Epoch 16/50\n",
      "4949/4949 [==============================] - 3s 656us/step - loss: 0.2836 - accuracy: 0.8775\n",
      "Epoch 17/50\n",
      "4949/4949 [==============================] - 3s 639us/step - loss: 0.2830 - accuracy: 0.8783\n",
      "Epoch 18/50\n",
      "4949/4949 [==============================] - 3s 645us/step - loss: 0.2828 - accuracy: 0.8777\n",
      "Epoch 19/50\n",
      "4949/4949 [==============================] - 3s 654us/step - loss: 0.2830 - accuracy: 0.8773\n",
      "Epoch 20/50\n",
      "4949/4949 [==============================] - 3s 641us/step - loss: 0.2827 - accuracy: 0.8780\n",
      "Epoch 21/50\n",
      "4949/4949 [==============================] - 3s 648us/step - loss: 0.2827 - accuracy: 0.8778\n",
      "Epoch 22/50\n",
      "4949/4949 [==============================] - 3s 646us/step - loss: 0.2825 - accuracy: 0.8779\n",
      "Epoch 23/50\n",
      "4949/4949 [==============================] - 3s 638us/step - loss: 0.2822 - accuracy: 0.8787\n",
      "Epoch 24/50\n",
      "4949/4949 [==============================] - 3s 652us/step - loss: 0.2820 - accuracy: 0.8770\n",
      "Epoch 25/50\n",
      "4949/4949 [==============================] - 3s 639us/step - loss: 0.2822 - accuracy: 0.8786\n",
      "Epoch 26/50\n",
      "4949/4949 [==============================] - 3s 642us/step - loss: 0.2821 - accuracy: 0.8774\n",
      "Epoch 27/50\n",
      "4949/4949 [==============================] - 3s 651us/step - loss: 0.2821 - accuracy: 0.8781\n",
      "Epoch 28/50\n",
      "4949/4949 [==============================] - 3s 649us/step - loss: 0.2819 - accuracy: 0.8793\n",
      "Epoch 29/50\n",
      "4949/4949 [==============================] - 3s 647us/step - loss: 0.2820 - accuracy: 0.8782\n",
      "Epoch 30/50\n",
      "4949/4949 [==============================] - 3s 652us/step - loss: 0.2819 - accuracy: 0.8782\n",
      "Epoch 31/50\n",
      "4949/4949 [==============================] - 3s 634us/step - loss: 0.2821 - accuracy: 0.8785\n",
      "Epoch 32/50\n",
      "4949/4949 [==============================] - 3s 648us/step - loss: 0.2815 - accuracy: 0.8778\n",
      "Epoch 33/50\n",
      "4949/4949 [==============================] - 3s 658us/step - loss: 0.2815 - accuracy: 0.8784\n",
      "Epoch 34/50\n",
      "4949/4949 [==============================] - 3s 649us/step - loss: 0.2814 - accuracy: 0.8793\n",
      "Epoch 35/50\n",
      "4949/4949 [==============================] - 3s 652us/step - loss: 0.2812 - accuracy: 0.8782\n",
      "Epoch 36/50\n",
      "4949/4949 [==============================] - 3s 648us/step - loss: 0.2814 - accuracy: 0.8785\n",
      "Epoch 37/50\n",
      "4949/4949 [==============================] - 3s 632us/step - loss: 0.2810 - accuracy: 0.8788\n",
      "Epoch 38/50\n",
      "4949/4949 [==============================] - 3s 635us/step - loss: 0.2811 - accuracy: 0.8786\n",
      "Epoch 39/50\n",
      "4949/4949 [==============================] - 3s 641us/step - loss: 0.2811 - accuracy: 0.8790\n",
      "Epoch 40/50\n",
      "4949/4949 [==============================] - 3s 643us/step - loss: 0.2810 - accuracy: 0.8785\n",
      "Epoch 41/50\n",
      "4949/4949 [==============================] - 3s 656us/step - loss: 0.2806 - accuracy: 0.8790\n",
      "Epoch 42/50\n",
      "4949/4949 [==============================] - 3s 636us/step - loss: 0.2805 - accuracy: 0.8782\n",
      "Epoch 43/50\n",
      "4949/4949 [==============================] - 3s 627us/step - loss: 0.2806 - accuracy: 0.8789\n",
      "Epoch 44/50\n",
      "4949/4949 [==============================] - 3s 639us/step - loss: 0.2805 - accuracy: 0.8790\n",
      "Epoch 45/50\n",
      "4949/4949 [==============================] - 3s 639us/step - loss: 0.2806 - accuracy: 0.8782\n",
      "Epoch 46/50\n",
      "4949/4949 [==============================] - 3s 635us/step - loss: 0.2804 - accuracy: 0.8782\n",
      "Epoch 47/50\n",
      "4949/4949 [==============================] - 3s 646us/step - loss: 0.2806 - accuracy: 0.8787\n",
      "Epoch 48/50\n",
      "4949/4949 [==============================] - 3s 640us/step - loss: 0.2802 - accuracy: 0.8788\n",
      "Epoch 49/50\n",
      "4949/4949 [==============================] - 3s 652us/step - loss: 0.2803 - accuracy: 0.8795\n",
      "Epoch 50/50\n",
      "4949/4949 [==============================] - 3s 649us/step - loss: 0.2806 - accuracy: 0.8789\n",
      "1547/1547 [==============================] - 1s 530us/step - loss: 0.2785 - accuracy: 0.8801\n",
      "663/663 [==============================] - 0s 532us/step - loss: 0.2909 - accuracy: 0.8755\n",
      "Train Accuracy: 0.8801228404045105\n",
      "Test Accuracy: 0.8755186796188354\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(20, input_shape=(9,), activation='relu'))\n",
    "nn.add(Dense(10, activation = 'relu'))\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn.compile(loss = 'binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "nn.fit(backward_selection_train, y_train.values.ravel(), epochs=50, batch_size=10)\n",
    "\n",
    "_, train_accuracy = nn.evaluate(backward_selection_train, y_train)\n",
    "_2, test_accuracy = nn.evaluate(backward_selection_test, y_test)\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e622c14e",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5ea19067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RandomForestClassifier(criterion = 'gini', max_depth = 10, min_samples_split = 18, random_state=42)\n",
    "model_2 = SVC(C = 5, degree = 3, gamma = 'scale', kernel = 'rbf')\n",
    "model_3 = LogisticRegression(C = 1, penalty = 'l1', random_state=42, solver = 'saga')\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators = [('rf', model_1), ('svm', model_2), ('lg', model_3)],\n",
    "                                 voting = 'hard',\n",
    "                                 n_jobs = -1,\n",
    "                                 verbose = True)\n",
    "ensemble_model.fit(X_selected_train, y_train.values.ravel())\n",
    "ensemble_pred_train = ensemble_model.predict(X_selected_train)\n",
    "ensemble_pred_test = ensemble_model.predict(X_selected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0fb3df36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8156777948427775\n",
      "Classification Report for Train: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.90      0.83     24745\n",
      "         1.0       0.88      0.73      0.80     24739\n",
      "\n",
      "    accuracy                           0.82     49484\n",
      "   macro avg       0.82      0.82      0.81     49484\n",
      "weighted avg       0.82      0.82      0.81     49484\n",
      "\n",
      "\n",
      "Test Accuracy: 0.8156827612221803\n",
      "Classification Report for Test: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.90      0.83     10601\n",
      "         1.0       0.88      0.73      0.80     10607\n",
      "\n",
      "    accuracy                           0.82     21208\n",
      "   macro avg       0.82      0.82      0.81     21208\n",
      "weighted avg       0.82      0.82      0.81     21208\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_train_score = accuracy_score(y_train.values.ravel(), ensemble_pred_train)\n",
    "ensemble_test_score = accuracy_score(y_test.values.ravel(), ensemble_pred_test)\n",
    "\n",
    "print(f'Train Accuracy: {ensemble_train_score}')\n",
    "print(f'Classification Report for Train: \\n{classification_report(y_train.values.ravel(), ensemble_pred_train)}\\n')\n",
    "\n",
    "print(f'Test Accuracy: {ensemble_test_score}')\n",
    "print(f'Classification Report for Test: \\n{classification_report(y_test.values.ravel(), ensemble_pred_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d5bc465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.877394713442729\n",
      "Classification Report for Train: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.89     24745\n",
      "         1.0       0.94      0.81      0.87     24739\n",
      "\n",
      "    accuracy                           0.88     49484\n",
      "   macro avg       0.88      0.88      0.88     49484\n",
      "weighted avg       0.88      0.88      0.88     49484\n",
      "\n",
      "\n",
      "Test Accuracy: 0.872736703130894\n",
      "Classification Report for Test: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88     10601\n",
      "         1.0       0.93      0.81      0.86     10607\n",
      "\n",
      "    accuracy                           0.87     21208\n",
      "   macro avg       0.88      0.87      0.87     21208\n",
      "weighted avg       0.88      0.87      0.87     21208\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1 = RandomForestClassifier(criterion = 'gini', max_depth = 10, min_samples_split = 18, random_state=42)\n",
    "model_2 = SVC(C = 5, degree = 3, gamma = 'scale', kernel = 'rbf')\n",
    "model_3 = LogisticRegression(C = 1, penalty = 'l1', random_state=42, solver = 'saga')\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators = [('rf', model_1), ('svm', model_2), ('lg', model_3)],\n",
    "                                 voting = 'hard',\n",
    "                                 n_jobs = -1,\n",
    "                                 verbose = True)\n",
    "ensemble_model.fit(backward_selection_train, y_train.values.ravel())\n",
    "ensemble_pred_train = ensemble_model.predict(backward_selection_train)\n",
    "ensemble_pred_test = ensemble_model.predict(backward_selection_test)\n",
    "\n",
    "ensemble_train_score = accuracy_score(y_train.values.ravel(), ensemble_pred_train)\n",
    "ensemble_test_score = accuracy_score(y_test.values.ravel(), ensemble_pred_test)\n",
    "\n",
    "print(f'Train Accuracy: {ensemble_train_score}')\n",
    "print(f'Classification Report for Train: \\n{classification_report(y_train.values.ravel(), ensemble_pred_train)}\\n')\n",
    "\n",
    "print(f'Test Accuracy: {ensemble_test_score}')\n",
    "print(f'Classification Report for Test: \\n{classification_report(y_test.values.ravel(), ensemble_pred_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397e70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
